{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script loads the current model and performs an evaluation of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "First, initialize the model with all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import pyshtools\n",
    "from pyshtools import spectralanalysis\n",
    "from pyshtools import shio\n",
    "from pyshtools import expand\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as st\n",
    "from scipy import spatial\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "net = Model().cuda()\n",
    "restore = False\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "batch_size = 12\n",
    "num_workers = 12\n",
    "descriptor_size = 256\n",
    "bandwidth = 100\n",
    "net_input_size = 2*bandwidth\n",
    "n_features = 3\n",
    "cache = 50\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "writer = SummaryWriter()\n",
    "stored_model = './net_params_phaser.pkl'\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "#summary(net, input_size=[(2, 200, 200), (2, 200, 200), (2, 200, 200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors from:\t/media/scratch/berlukas/test/training_anchor_pointclouds/ and /media/scratch/berlukas/test/training_anchor_sph_images/\n",
      "Loading positives from:\t/media/scratch/berlukas/test/training_positive_pointclouds/ and /media/scratch/berlukas/test/training_positive_sph_images/\n",
      "Loading negatives from:\t/media/scratch/berlukas/test/training_negative_pointclouds/ and /media/scratch/berlukas/test/training_negative_sph_images/\n",
      "Done loading dataset.\n",
      "\tAnchor point clouds total: \t127\n",
      "\tAnchor images total: \t\t127\n",
      "\tAnchor poses total: \t\t127\n",
      "\tPositive point clouds total: \t127\n",
      "\tPositive images total: \t\t127\n",
      "\tPositive poses total: \t\t127\n",
      "\tNegative point clouds total: \t127\n",
      "\tNegative images total: \t\t127\n",
      "\tNegative poses total: \t\t127\n"
     ]
    }
   ],
   "source": [
    "n_data = 127\n",
    "ds = DataSource('/media/scratch/berlukas/test', n_data, -1)\n",
    "ds.load(n_data)\n",
    "n_data = len(ds.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c9c046e8e64f6b97eb147a35fdac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating positive spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665de72c4e15428a8603d14ddfb76499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating negative spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc325ddfd3d4d4db9497e8843259dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=127.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated features\n",
      "Total size:  127\n"
     ]
    }
   ],
   "source": [
    "test_set = TrainingSet(restore, bandwidth)\n",
    "test_set.generateAll(ds)\n",
    "print(\"Total size: \", len(test_set))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, shuffle=False, num_workers=1, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate the descriptors for anchor and positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    return acc\n",
    "\n",
    "net.eval()\n",
    "n_iter = 0\n",
    "anchor_embeddings = np.empty(1)\n",
    "positive_embeddings = np.empty(1)\n",
    "with torch.no_grad():\n",
    "    test_accs = AverageMeter()\n",
    "    test_pos_dist = AverageMeter()\n",
    "    test_neg_dist = AverageMeter()\n",
    "\n",
    "    for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data3.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        writer.add_scalar('Ext_Test/Loss', loss, n_iter)\n",
    "\n",
    "        acc = accuracy(dist_to_pos, dist_to_neg)\n",
    "        test_accs.update(acc, data1.size(0))\n",
    "        test_pos_dist.update(dist_to_pos.cpu().data.numpy().sum())\n",
    "        test_neg_dist.update(dist_to_neg.cpu().data.numpy().sum())\n",
    "\n",
    "        writer.add_scalar('Ext_Test/Accuracy', test_accs.avg, n_iter)\n",
    "        writer.add_scalar('Ext_Test/Distance/Positive', test_pos_dist.avg, n_iter)\n",
    "        writer.add_scalar('Ext_Test/Distance/Negative', test_neg_dist.avg, n_iter)\n",
    "\n",
    "        anchor_embeddings = np.append(anchor_embeddings, embedded_a.cpu().data.numpy().reshape([1,-1]))\n",
    "        positive_embeddings = np.append(positive_embeddings, embedded_p.cpu().data.numpy().reshape([1,-1]))\n",
    "        n_iter = n_iter + 1\n",
    "        \n",
    "desc_anchors = anchor_embeddings[1:].reshape([n_data, descriptor_size])\n",
    "desc_positives = positive_embeddings[1:].reshape([n_data, descriptor_size])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple old testing pipeline (index based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1102c89b7ad4831b8a86df8499cd513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.047244094488188976 for 1 neighbors\n",
      "recall 0.10236220472440945 for 2 neighbors\n",
      "recall 0.2047244094488189 for 3 neighbors\n",
      "recall 0.28346456692913385 for 4 neighbors\n",
      "recall 0.33070866141732286 for 5 neighbors\n",
      "recall 0.36220472440944884 for 6 neighbors\n",
      "recall 0.3937007874015748 for 7 neighbors\n",
      "recall 0.4251968503937008 for 8 neighbors\n",
      "recall 0.47244094488188976 for 9 neighbors\n",
      "recall 0.5118110236220472 for 10 neighbors\n",
      "recall 0.5275590551181102 for 11 neighbors\n",
      "recall 0.5433070866141733 for 12 neighbors\n",
      "recall 0.5511811023622047 for 13 neighbors\n",
      "recall 0.5669291338582677 for 14 neighbors\n",
      "recall 0.5669291338582677 for 15 neighbors\n",
      "recall 0.5905511811023622 for 16 neighbors\n",
      "recall 0.5905511811023622 for 17 neighbors\n",
      "recall 0.6062992125984252 for 18 neighbors\n",
      "recall 0.6220472440944882 for 19 neighbors\n",
      "recall 0.6299212598425197 for 20 neighbors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 0.05\n",
    "max_anchor_dist = 1\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):\n",
    "    pos_count = 0\n",
    "    anchor_count = 0\n",
    "    idx_count = 0\n",
    "    for idx in range(n_data):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_data):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_positives[idx,:])\n",
    "            if (dist <= max_pos_dist):\n",
    "                pos_count = pos_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_data):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_anchors[idx,:])\n",
    "            if (dist <= max_anchor_dist):\n",
    "                anchor_count = anchor_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i == idx):\n",
    "                idx_count = idx_count + 1;\n",
    "                break\n",
    "    pos_precision = (pos_count*1.0) / n_data\n",
    "    anchor_precision = (anchor_count*1.0) / n_data\n",
    "    idx_precision = (idx_count*1.0) / n_data\n",
    "    \n",
    "    print(f'recall {idx_precision} for {n_nearest_neighbors} neighbors')\n",
    "    writer.add_scalar('Ext_Test/Precision/Positive_Distance', pos_precision, n_nearest_neighbors)\n",
    "    writer.add_scalar('Ext_Test/Precision/Anchor_Distance', anchor_precision, n_nearest_neighbors)\n",
    "    writer.add_scalar('Ext_Test/Precision/Index_Count', idx_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New testing pipeline (location based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 127 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f410f371954b86a03f0f1125df3926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637795275590551\n",
      "0.7874015748031497\n",
      "0.84251968503937\n",
      "0.8661417322834646\n",
      "0.8661417322834646\n",
      "0.905511811023622\n",
      "0.905511811023622\n",
      "0.905511811023622\n",
      "0.9212598425196851\n",
      "0.9212598425196851\n",
      "0.9212598425196851\n",
      "0.9291338582677166\n",
      "0.937007874015748\n",
      "0.937007874015748\n",
      "0.937007874015748\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "max_anchor_dist = 1\n",
    "anchor_poses = ds.anchor_poses\n",
    "positive_poses = ds.positive_poses\n",
    "assert len(anchor_poses) == len(positive_poses)\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):    \n",
    "    loc_count = 0\n",
    "    for idx in range(n_data):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_data):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                loc_count = loc_count + 1;\n",
    "                break\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_data    \n",
    "    #print(f'recall {loc_precision} for {n_nearest_neighbors} neighbors')\n",
    "    print(f'{loc_precision}')\n",
    "    writer.add_scalar('Ext_Test/Precision/Location', loc_precision, n_nearest_neighbors)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 127 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bcd689602048a2b49fba0527f69182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9565217391304348\n",
      "0.9482758620689655\n",
      "0.9230769230769231\n",
      "0.940677966101695\n",
      "0.9327731092436975\n",
      "0.9327731092436975\n",
      "0.9256198347107438\n",
      "0.9262295081967213\n",
      "0.9344262295081968\n",
      "0.9426229508196722\n",
      "0.9508196721311475\n",
      "0.9508196721311475\n",
      "0.9512195121951219\n",
      "0.9512195121951219\n",
      "0.959349593495935\n",
      "0.9596774193548387\n",
      "0.9596774193548387\n",
      "0.9596774193548387\n",
      "0.9596774193548387\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds.anchor_poses\n",
    "anchor_clouds = ds.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds.positive_poses\n",
    "positive_clouds = ds.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):        \n",
    "    n_matches = 0\n",
    "    loc_count = 0    \n",
    "    for idx in range(0, n_data):        \n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        \n",
    "        z_scores = [0] * n_nearest_neighbors\n",
    "        contains_match = False        \n",
    "        true_match_idx = 0\n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_data):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True                \n",
    "                true_match_idx = i\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "                                               \n",
    "            a_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            \n",
    "            admit, error, corr = spectralanalysis.SHAdmitCorr(a_coeffs, p_coeffs)            \n",
    "            for l in range(0, 4):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores[i] = z_scores[i] + score\n",
    "                #if math.isinf(z_scores[i]):\n",
    "                    #print(f'z-score is inf: prob = {prob}, z-score {st.norm.ppf(1-(1-prob)/2)}')\n",
    "            \n",
    "        if (contains_match is not True):\n",
    "            #print(f'Match not found for index {idx} and {n_nearest_neighbors} neighbors')\n",
    "            continue\n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "        max_index, max_z_score = max(enumerate(z_scores), key=operator.itemgetter(1))\n",
    "        matching_index = nn_indices[max_index]\n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "        else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_z_score}.')            \n",
    "            matching_index = nn_indices[true_match_idx]\n",
    "            dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    writer.add_scalar('Ext_Test/Precision/Voting', loc_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 127 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea02d0af0ea446bb77afd3ad435be04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9739130434782609\n",
      "0.9568965517241379\n",
      "0.9487179487179487\n",
      "0.9830508474576272\n",
      "0.9831932773109243\n",
      "0.9831932773109243\n",
      "0.9834710743801653\n",
      "0.9836065573770492\n",
      "0.9836065573770492\n",
      "0.9836065573770492\n",
      "0.9836065573770492\n",
      "0.9754098360655737\n",
      "0.967479674796748\n",
      "0.967479674796748\n",
      "0.967479674796748\n",
      "0.967741935483871\n",
      "0.967741935483871\n",
      "0.967741935483871\n",
      "0.967741935483871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds.anchor_poses\n",
    "anchor_clouds = ds.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds.positive_poses\n",
    "positive_clouds = ds.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):        \n",
    "    n_matches = 0\n",
    "    loc_count = 0    \n",
    "    for idx in range(0, n_data):        \n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        \n",
    "        z_scores = [0] * n_nearest_neighbors\n",
    "        contains_match = False        \n",
    "        true_match_idx = 0\n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_data):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True                \n",
    "                true_match_idx = i\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "                                               \n",
    "            a_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            tapers, eigenvalues, taper_order = spectralanalysis.SHReturnTapers(2.01, 1)\n",
    "            saa = spectralanalysis.spectrum(a_coeffs)\n",
    "            spp = spectralanalysis.spectrum(p_coeffs)\n",
    "            sap = spectralanalysis.cross_spectrum(a_coeffs, p_coeffs)\n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap, saa, spp, tapers)\n",
    "                        \n",
    "            \n",
    "            for l in range(0, 4):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores[i] = z_scores[i] + score\n",
    "                #if math.isinf(z_scores[i]):\n",
    "                    #print(f'z-score is inf: prob = {prob}, z-score {st.norm.ppf(1-(1-prob)/2)}')\n",
    "            \n",
    "        if (contains_match is not True):\n",
    "            #print(f'Match not found for index {idx} and {n_nearest_neighbors} neighbors')\n",
    "            continue\n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "        max_index, max_z_score = max(enumerate(z_scores), key=operator.itemgetter(1))\n",
    "        matching_index = nn_indices[max_index]\n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "        else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_z_score}.')            \n",
    "            matching_index = nn_indices[true_match_idx]\n",
    "            dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    writer.add_scalar('Ext_Test/Precision/WindowedVoting', loc_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
