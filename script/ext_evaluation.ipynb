{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script loads the current model and performs an evaluation of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "First, initialize the model with all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from model_relu_old import ModelOld\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "from mission_indices import MissionIndices\n",
    "from database_parser import DatabaseParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import pyshtools\n",
    "from pyshtools import spectralanalysis\n",
    "from pyshtools import shio\n",
    "from pyshtools import expand\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as st\n",
    "from scipy import spatial\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     S2Convolution-1    [-1, 10, 100, 100, 100]              10\n",
      "             PReLU-2    [-1, 10, 100, 100, 100]               1\n",
      "       BatchNorm3d-3    [-1, 10, 100, 100, 100]              20\n",
      "    SO3Convolution-4       [-1, 20, 80, 80, 80]              20\n",
      "             PReLU-5       [-1, 20, 80, 80, 80]               1\n",
      "       BatchNorm3d-6       [-1, 20, 80, 80, 80]              40\n",
      "    SO3Convolution-7       [-1, 60, 60, 60, 60]              60\n",
      "             PReLU-8       [-1, 60, 60, 60, 60]               1\n",
      "       BatchNorm3d-9       [-1, 60, 60, 60, 60]             120\n",
      "   SO3Convolution-10      [-1, 100, 40, 40, 40]             100\n",
      "      BatchNorm3d-11      [-1, 100, 40, 40, 40]             200\n",
      "            PReLU-12      [-1, 100, 40, 40, 40]               1\n",
      "   SO3Convolution-13      [-1, 200, 10, 10, 10]             200\n",
      "      BatchNorm3d-14      [-1, 200, 10, 10, 10]             400\n",
      "            PReLU-15      [-1, 200, 10, 10, 10]               1\n",
      "      BatchNorm1d-16                  [-1, 200]             400\n",
      "           Linear-17                  [-1, 512]         102,912\n",
      "            PReLU-18                  [-1, 512]               1\n",
      "          Dropout-19                  [-1, 512]               0\n",
      "      BatchNorm1d-20                  [-1, 512]           1,024\n",
      "           Linear-21                  [-1, 256]         131,328\n",
      "            PReLU-22                  [-1, 256]               1\n",
      "          Dropout-23                  [-1, 256]               0\n",
      "      BatchNorm1d-24                  [-1, 256]             512\n",
      "           Linear-25                  [-1, 256]          65,792\n",
      "    S2Convolution-26    [-1, 10, 100, 100, 100]              10\n",
      "            PReLU-27    [-1, 10, 100, 100, 100]               1\n",
      "      BatchNorm3d-28    [-1, 10, 100, 100, 100]              20\n",
      "   SO3Convolution-29       [-1, 20, 80, 80, 80]              20\n",
      "            PReLU-30       [-1, 20, 80, 80, 80]               1\n",
      "      BatchNorm3d-31       [-1, 20, 80, 80, 80]              40\n",
      "   SO3Convolution-32       [-1, 60, 60, 60, 60]              60\n",
      "            PReLU-33       [-1, 60, 60, 60, 60]               1\n",
      "      BatchNorm3d-34       [-1, 60, 60, 60, 60]             120\n",
      "   SO3Convolution-35      [-1, 100, 40, 40, 40]             100\n",
      "      BatchNorm3d-36      [-1, 100, 40, 40, 40]             200\n",
      "            PReLU-37      [-1, 100, 40, 40, 40]               1\n",
      "   SO3Convolution-38      [-1, 200, 10, 10, 10]             200\n",
      "      BatchNorm3d-39      [-1, 200, 10, 10, 10]             400\n",
      "            PReLU-40      [-1, 200, 10, 10, 10]               1\n",
      "      BatchNorm1d-41                  [-1, 200]             400\n",
      "           Linear-42                  [-1, 512]         102,912\n",
      "            PReLU-43                  [-1, 512]               1\n",
      "          Dropout-44                  [-1, 512]               0\n",
      "      BatchNorm1d-45                  [-1, 512]           1,024\n",
      "           Linear-46                  [-1, 256]         131,328\n",
      "            PReLU-47                  [-1, 256]               1\n",
      "          Dropout-48                  [-1, 256]               0\n",
      "      BatchNorm1d-49                  [-1, 256]             512\n",
      "           Linear-50                  [-1, 256]          65,792\n",
      "    S2Convolution-51    [-1, 10, 100, 100, 100]              10\n",
      "            PReLU-52    [-1, 10, 100, 100, 100]               1\n",
      "      BatchNorm3d-53    [-1, 10, 100, 100, 100]              20\n",
      "   SO3Convolution-54       [-1, 20, 80, 80, 80]              20\n",
      "            PReLU-55       [-1, 20, 80, 80, 80]               1\n",
      "      BatchNorm3d-56       [-1, 20, 80, 80, 80]              40\n",
      "   SO3Convolution-57       [-1, 60, 60, 60, 60]              60\n",
      "            PReLU-58       [-1, 60, 60, 60, 60]               1\n",
      "      BatchNorm3d-59       [-1, 60, 60, 60, 60]             120\n",
      "   SO3Convolution-60      [-1, 100, 40, 40, 40]             100\n",
      "      BatchNorm3d-61      [-1, 100, 40, 40, 40]             200\n",
      "            PReLU-62      [-1, 100, 40, 40, 40]               1\n",
      "   SO3Convolution-63      [-1, 200, 10, 10, 10]             200\n",
      "      BatchNorm3d-64      [-1, 200, 10, 10, 10]             400\n",
      "            PReLU-65      [-1, 200, 10, 10, 10]               1\n",
      "      BatchNorm1d-66                  [-1, 200]             400\n",
      "           Linear-67                  [-1, 512]         102,912\n",
      "            PReLU-68                  [-1, 512]               1\n",
      "          Dropout-69                  [-1, 512]               0\n",
      "      BatchNorm1d-70                  [-1, 512]           1,024\n",
      "           Linear-71                  [-1, 256]         131,328\n",
      "            PReLU-72                  [-1, 256]               1\n",
      "          Dropout-73                  [-1, 256]               0\n",
      "      BatchNorm1d-74                  [-1, 256]             512\n",
      "           Linear-75                  [-1, 256]          65,792\n",
      "================================================================\n",
      "Total params: 909,435\n",
      "Trainable params: 908,265\n",
      "Non-trainable params: 1,170\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 6591796875.00\n",
      "Forward/backward pass size (MB): 2732.93\n",
      "Params size (MB): 3.47\n",
      "Estimated Total Size (MB): 6591799611.40\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "n_features = 3\n",
    "bandwidth = 100\n",
    "from model_relu_old import ModelOld\n",
    "net = Model(n_features, bandwidth).cuda()\n",
    "#net = ModelOld(n_features, bandwidth).cuda()\n",
    "restore = False\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "batch_size = 12\n",
    "num_workers = 12\n",
    "descriptor_size = 256\n",
    "net_input_size = 2*bandwidth\n",
    "cache = 50\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "writer = SummaryWriter()\n",
    "stored_model = './net_params_arche_high_res_big.pkl'\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "summary(net, input_size=[(3, 200, 200), (3, 200, 200), (3, 200, 200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading missions db from /home/berlukas/data/arche_low_res2/missions.csv\n",
      "Read 21253 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004bcc7dabed4bc4810f31de6b9d1875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd7768927954dc1b0233fb3dc6e4359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2 test indices.\n",
      "Loading anchors from:\t/home/berlukas/data/arche_low_res2//training_anchor_pointclouds/ and /home/berlukas/data/arche_low_res2//training_anchor_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ff142999ca4b25affffa252959e782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362fc2abdbf6493dbea733c37315dc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading positives from:\t/home/berlukas/data/arche_low_res2//training_positive_pointclouds/ and /home/berlukas/data/arche_low_res2//training_positive_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff53b1a4bdec46509c0711e827a78d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f7c75e722b48c9bd371cda7c01fc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done loading dataset.\n",
      "\tAnchor point clouds total: \t1401\n",
      "\tAnchor images total: \t\t1401\n",
      "\tAnchor poses total: \t\t1401\n",
      "\tPositive point clouds total: \t1401\n",
      "\tPositive images total: \t\t1401\n",
      "\tPositive poses total: \t\t1401\n"
     ]
    }
   ],
   "source": [
    "#dataset_path = \"/media/scratch/berlukas/spherical/arche_high_res2/\"\n",
    "dataset_path = \"/home/berlukas/data/arche_low_res2/\"\n",
    "db_parser = DatabaseParser(dataset_path)\n",
    "\n",
    "training_missions, test_missions = MissionIndices.get_arche_low_res()\n",
    "training_indices, test_indices = db_parser.extract_training_and_test_indices(\n",
    "    training_missions, test_missions)\n",
    "print(f'Found {len(test_missions)} test indices.')\n",
    "\n",
    "n_test_data = 10500\n",
    "n_test_cache = n_test_data\n",
    "ds_test = DataSource(dataset_path, n_test_cache, -1, False)\n",
    "idx = np.array(test_indices['idx'].tolist())\n",
    "ds_test.load(n_test_data, idx, filter_clusters=True)\n",
    "n_test_data = len(ds_test.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features from 0 to 1401\n",
      "Generating anchor spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c739651af67d4710a19b6b1bf3dc528b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing time in total 276.6589322090149 for 1401 anchors.\n",
      "Generating positive spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c09d7f7d2cb4fcfa6889a2777c99b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1401.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing time in total 556.8650805950165 for 2802 positives.\n",
      "Generated all pcl features\n",
      "Processing time in total 556.8650805950165 for 2802 items.\n",
      "Processing time avg is 0.19874\n"
     ]
    }
   ],
   "source": [
    "test_set = TrainingSet(restore, bandwidth)\n",
    "test_set.generateAll(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size:  1401\n"
     ]
    }
   ],
   "source": [
    "# hack for removing the images\n",
    "#test_set.anchor_features = test_set.anchor_features[:,0:2,:,:]\n",
    "#test_set.positive_features = test_set.positive_features[:,0:2,:,:]\n",
    "#test_set.negative_features = test_set.negative_features[:,0:2,:,:]\n",
    "\n",
    "\n",
    "n_test_set = len(test_set)\n",
    "print(\"Total size: \", n_test_set)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate the descriptors for anchor and positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration per batch 0.13664241606299152s\n"
     ]
    }
   ],
   "source": [
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    return acc\n",
    "\n",
    "net.eval()\n",
    "n_iter = 0\n",
    "anchor_embeddings = np.empty(1)\n",
    "positive_embeddings = np.empty(1)\n",
    "with torch.no_grad():\n",
    "    test_accs = AverageMeter()\n",
    "    test_pos_dist = AverageMeter()\n",
    "    test_neg_dist = AverageMeter()\n",
    "    start = time.time()\n",
    "    for batch_idx, (data1, data2) in enumerate(test_loader):\n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data2.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        #writer.add_scalar('Ext_Test/Loss', loss, n_iter)\n",
    "\n",
    "        acc = accuracy(dist_to_pos, dist_to_neg)\n",
    "        test_accs.update(acc, data1.size(0))\n",
    "        test_pos_dist.update(dist_to_pos.cpu().data.numpy().sum())\n",
    "        test_neg_dist.update(dist_to_neg.cpu().data.numpy().sum())\n",
    "\n",
    "        #writer.add_scalar('Ext_Test/Accuracy', test_accs.avg, n_iter)\n",
    "        #writer.add_scalar('Ext_Test/Distance/Positive', test_pos_dist.avg, n_iter)\n",
    "        #writer.add_scalar('Ext_Test/Distance/Negative', test_neg_dist.avg, n_iter)\n",
    "\n",
    "        anchor_embeddings = np.append(anchor_embeddings, embedded_a.cpu().data.numpy().reshape([1,-1]))\n",
    "        positive_embeddings = np.append(positive_embeddings, embedded_p.cpu().data.numpy().reshape([1,-1]))\n",
    "        n_iter = n_iter + 1\n",
    "    end = time.time()\n",
    "    print(f'Duration per batch {(end - start)/n_test_set}s')    \n",
    "desc_anchors = anchor_embeddings[1:].reshape([n_test_set, descriptor_size])\n",
    "desc_positives = positive_embeddings[1:].reshape([n_test_set, descriptor_size])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0459082032995728"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1377246098987184/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple old testing pipeline (index based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef256cd8023420d830fdcc781726a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.022222222222222223 for 1 neighbors\n",
      "recall 0.044444444444444446 for 2 neighbors\n",
      "recall 0.06666666666666667 for 3 neighbors\n",
      "recall 0.08888888888888889 for 4 neighbors\n",
      "recall 0.1111111111111111 for 5 neighbors\n",
      "recall 0.13333333333333333 for 6 neighbors\n",
      "recall 0.15555555555555556 for 7 neighbors\n",
      "recall 0.17777777777777778 for 8 neighbors\n",
      "recall 0.2 for 9 neighbors\n",
      "recall 0.2222222222222222 for 10 neighbors\n",
      "recall 0.24444444444444444 for 11 neighbors\n",
      "recall 0.26666666666666666 for 12 neighbors\n",
      "recall 0.28888888888888886 for 13 neighbors\n",
      "recall 0.3111111111111111 for 14 neighbors\n",
      "recall 0.3333333333333333 for 15 neighbors\n",
      "recall 0.35555555555555557 for 16 neighbors\n",
      "recall 0.37777777777777777 for 17 neighbors\n",
      "recall 0.4 for 18 neighbors\n",
      "recall 0.4222222222222222 for 19 neighbors\n",
      "recall 0.4444444444444444 for 20 neighbors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 0.05\n",
    "max_anchor_dist = 1\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):\n",
    "    pos_count = 0\n",
    "    anchor_count = 0\n",
    "    idx_count = 0\n",
    "    for idx in range(n_test_set):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_positives[idx,:])\n",
    "            if (dist <= max_pos_dist):\n",
    "                pos_count = pos_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_anchors[idx,:])\n",
    "            if (dist <= max_anchor_dist):\n",
    "                anchor_count = anchor_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i == idx):\n",
    "                idx_count = idx_count + 1;\n",
    "                break\n",
    "    pos_precision = (pos_count*1.0) / n_test_set\n",
    "    anchor_precision = (anchor_count*1.0) / n_test_set\n",
    "    idx_precision = (idx_count*1.0) / n_test_set\n",
    "    \n",
    "    print(f'recall {idx_precision} for {n_nearest_neighbors} neighbors')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Positive_Distance', pos_precision, n_nearest_neighbors)\n",
    "    #writer.add_scalar('Ext_Test/Precision/Anchor_Distance', anchor_precision, n_nearest_neighbors)\n",
    "    #writer.add_scalar('Ext_Test/Precision/Index_Count', idx_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New testing pipeline (location based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 877 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1cd18558e0e46258aee7056d46d5eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7651083238312428\n",
      "0.8004561003420753\n",
      "0.8278221208665907\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-c2a1c661f5c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloc_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mnn_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc_anchors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_nearest_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mnn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn_nearest_neighbors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, x, k, eps, p, distance_upper_bound)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mhits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, x, k, eps, p, distance_upper_bound)\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;31m# brute-force\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminkowski_distance_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdistance_upper_bound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/scipy/spatial/kdtree.py\u001b[0m in \u001b[0;36mminkowski_distance_p\u001b[0;34m(x, y, p)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "max_anchor_dist = 1\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "positive_poses = ds_test.positive_poses\n",
    "assert len(anchor_poses) == len(positive_poses)\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):    \n",
    "    loc_count = 0\n",
    "    for idx in range(0, n_test_set):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                loc_count = loc_count + 1;\n",
    "                break\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_test_set    \n",
    "    #print(f'recall {loc_precision} for {n_nearest_neighbors} neighbors')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Location', loc_precision, n_nearest_neighbors)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 877 descriptors.\n",
      "Duration for building the kd-tree 0.07774639129638672s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d208a9821f47709563b235ee67b731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-6d59af6dbb40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m#p_img = positive_features[nn_i][2,:,:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0ma_range_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyshtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSHExpandDH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mp_range_coeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyshtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSHExpandDH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/pyshtools/shtools/__init__.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_errors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mreturned_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturned_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSHToolsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shtools_status_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturned_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,2)):        \n",
    "    n_matches = 0\n",
    "    loc_count = 0    \n",
    "    dur_neighbor_processing_s = 0\n",
    "    for idx in range(0, n_test_set):                \n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        \n",
    "        z_scores = [0] * n_nearest_neighbors\n",
    "        contains_match = False        \n",
    "        true_match_idx = 0\n",
    "        start = time.time()\n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_test_set):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True                \n",
    "                true_match_idx = i\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "            a_intensity = anchor_features[idx][1,:,:]\n",
    "            p_intensity = positive_features[nn_i][1,:,:]\n",
    "            #a_img = anchor_features[idx][2,:,:]\n",
    "            #p_img = positive_features[nn_i][2,:,:]\n",
    "                                               \n",
    "            a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            \n",
    "            a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "            p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "            \n",
    "            #a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "            #p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "            \n",
    "            #a_fused = np.empty([3, a_range_coeffs.shape[0], a_range_coeffs.shape[1]])\n",
    "            #p_fused = np.empty([3, p_range_coeffs.shape[0], p_range_coeffs.shape[1]])\n",
    "            #print(a_range_coeffs.shape)\n",
    "            #a_fused[0,:] = a_range_coeffs\n",
    "            \n",
    "            \n",
    "            admit, error, corr = spectralanalysis.SHAdmitCorr(a_range_coeffs, p_range_coeffs)            \n",
    "            for l in range(0, 4):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores[i] = z_scores[i] + score\n",
    "                #if math.isinf(z_scores[i]):\n",
    "                    #print(f'z-score is inf: prob = {prob}, z-score {st.norm.ppf(1-(1-prob)/2)}')\n",
    "            \n",
    "        #if (contains_match is not True):\n",
    "            #print(f'Match not found for index {idx} and {n_nearest_neighbors} neighbors')\n",
    "            #continue\n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "        max_index, max_z_score = max(enumerate(z_scores), key=operator.itemgetter(1))\n",
    "        matching_index = nn_indices[max_index]\n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "        else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_z_score}.')            \n",
    "            matching_index = nn_indices[true_match_idx]\n",
    "            dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Voting', loc_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 1401 descriptors.\n",
      "Duration for building the kd-tree 0.07299685478210449s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa10a13c7c5b4853a96068f828cff3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=19.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924812030075188\n",
      "0.8918367346938776\n",
      "0.8714703018500487\n",
      "0.8584729981378026\n",
      "0.8540145985401459\n",
      "0.8432769367764915\n",
      "0.8466257668711656\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-34e394b87d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mstart_spectrum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msaa_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectralanalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_range_coeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0msaa_intensity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectralanalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_intensity_coeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0msaa_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectralanalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspectrum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_img_coeffs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/pyshtools/spectralanalysis/spectrum.py\u001b[0m in \u001b[0;36mspectrum\u001b[0;34m(clm, normalization, degrees, lmax, convention, unit, base)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                            \u001b[0;34m(\u001b[0m\u001b[0mclm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l2norm'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/python/pytorch-venv/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     45\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     46\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "tapers, eigenvalues, taper_order = spectralanalysis.SHReturnTapers(10.3, 7)\n",
    "n_bands = 7\n",
    "for n_nearest_neighbors in tqdm(range(2,21)):        \n",
    "    n_matches = 0    \n",
    "    loc_count = 0    \n",
    "    final_count = 0\n",
    "    dur_neighbor_processing_s = 0\n",
    "    dur_s2_s = 0\n",
    "    dur_spectrum_s = 0\n",
    "    for idx in range(0, n_test_set):        \n",
    "    #for idx in range(0, 100):        \n",
    "        start = time.time()\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)                \n",
    "        end = time.time()\n",
    "        dur_neighbor_processing_s = dur_neighbor_processing_s + (end - start)\n",
    "        \n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        z_scores_fused = [0] * n_nearest_neighbors\n",
    "        z_scores_range = [0] * n_nearest_neighbors\n",
    "        z_scores_intensity = [0] * n_nearest_neighbors\n",
    "        z_scores_image = [0] * n_nearest_neighbors                     \n",
    "        n_true_matches = 0   \n",
    "        contains_match = False        \n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_test_set):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True   \n",
    "                n_true_matches = n_true_matches + 1       \n",
    "                #print(f'True index = {i}')\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "            a_intensity = anchor_features[idx][1,:,:]\n",
    "            p_intensity = positive_features[nn_i][1,:,:]\n",
    "            a_img = anchor_features[idx][2,:,:]\n",
    "            p_img = positive_features[nn_i][2,:,:]\n",
    "                                                 \n",
    "            start_s2 = time.time()\n",
    "            a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            \n",
    "            a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "            p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "            \n",
    "            a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "            p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "            end_s2 = time.time()\n",
    "            dur_s2_s = dur_s2_s + (end_s2 - start_s2)\n",
    "                        \n",
    "                \n",
    "            start_spectrum = time.time()\n",
    "            saa_range = spectralanalysis.spectrum(a_range_coeffs)            \n",
    "            saa_intensity = spectralanalysis.spectrum(a_intensity_coeffs)    \n",
    "            saa_img = spectralanalysis.spectrum(a_img_coeffs)    \n",
    "            saa = np.empty([n_features, saa_range.shape[0]])\n",
    "            saa[0,:] = saa_range\n",
    "            saa[1,:] = saa_intensity\n",
    "            saa[2,:] = saa_img\n",
    "            #saa = np.mean(saa, axis=0)\n",
    "            saa = np.amax(saa, axis=0)\n",
    "            \n",
    "            spp_range = spectralanalysis.spectrum(p_range_coeffs)            \n",
    "            spp_intensity = spectralanalysis.spectrum(p_intensity_coeffs)    \n",
    "            spp_img = spectralanalysis.spectrum(p_img_coeffs)    \n",
    "            spp = np.empty([n_features, spp_range.shape[0]])\n",
    "            spp[0,:] = spp_range\n",
    "            spp[1,:] = spp_intensity\n",
    "            spp[2,:] = spp_img\n",
    "            #spp = np.mean(spp, axis=0)\n",
    "            spp = np.amax(spp, axis=0)\n",
    "            \n",
    "            sap_range = spectralanalysis.cross_spectrum(a_range_coeffs, p_range_coeffs)            \n",
    "            sap_intensity = spectralanalysis.cross_spectrum(a_intensity_coeffs, p_intensity_coeffs)    \n",
    "            sap_img = spectralanalysis.cross_spectrum(a_img_coeffs, p_img_coeffs)    \n",
    "            sap = np.empty([n_features, sap_range.shape[0]])\n",
    "            sap[0,:] = sap_range\n",
    "            sap[1,:] = sap_intensity\n",
    "            sap[2,:] = sap_img\n",
    "            #sap = np.mean(sap, axis=0)\n",
    "            sap = np.amax(sap, axis=0)\n",
    "            \n",
    "            #saa = spectralanalysis.spectrum(a_coeffs)\n",
    "            #spp = spectralanalysis.spectrum(p_coeffs)\n",
    "            #sap = spectralanalysis.cross_spectrum(a_coeffs, p_coeffs)\n",
    "            \n",
    "            #admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                                    \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap, saa, spp, tapers)\n",
    "            end_spectrum = time.time()\n",
    "            dur_spectrum_s = dur_spectrum_s + (end_spectrum - start_spectrum)\n",
    "            \n",
    "            \n",
    "            for l in range(0, 10):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_fused[i] = z_scores_fused[i] + score  \n",
    "            \n",
    "                             \n",
    "\n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_range, saa_range, spp_range, tapers)                        \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_range[i] = z_scores_range[i] + score\n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_intensity, saa_intensity, spp_intensity, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_intensity[i] = z_scores_intensity[i] + score                           \n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_image[i] = z_scores_image[i] + score                                                                   \n",
    "            \n",
    "            \n",
    "        if (contains_match is not True):            \n",
    "            continue\n",
    "        \n",
    "                \n",
    "        #print(f'z_score > 2 = {np.sum(np.array(z_scores_range) > 3.8)} range, {np.sum(np.array(z_scores_intensity) > 20)} intensity')\n",
    "        #print(f'true matches: {n_true_matches}')\n",
    "                \n",
    "        \n",
    "        # normalize values\n",
    "        z_scores_fused = np.array(z_scores_fused) / (n_bands)\n",
    "        z_scores_range = np.array(z_scores_range) / (n_bands)\n",
    "        z_scores_intensity = np.array(z_scores_intensity) / (n_bands)\n",
    "        z_scores_image = np.array(z_scores_image) / (n_bands)\n",
    "        \n",
    "#        print(z_scores_range)\n",
    " #       print(z_scores_intensity)\n",
    "  #      print(z_scores_image)\n",
    "        \n",
    "        \n",
    "        max_index_fused, max_z_score_fused = max(enumerate(z_scores_fused), key=operator.itemgetter(1))\n",
    "        max_index_range, max_z_score_range = max(enumerate(z_scores_range), key=operator.itemgetter(1))\n",
    "        max_index_intensity, max_z_score_intensity = max(enumerate(z_scores_intensity), key=operator.itemgetter(1))        \n",
    "        max_index_image, max_z_score_image = max(enumerate(z_scores_image), key=operator.itemgetter(1))\n",
    "        \n",
    "        #score_mean = np.mean([z_scores_range,z_scores_intensity,z_scores_image], axis=0)\n",
    "        #print(f'score_mean: {score_mean}')\n",
    "        #conf_mask = np.logical_or(np.logical_or(z_scores_range > 3.5, z_scores_intensity > 3.5), z_scores_image > 3.5)\n",
    "        #print(f'conf_mask: {conf_mask}')\n",
    "        #if len(score_mean[conf_mask])==0:\n",
    "#            continue\n",
    "#        idx_map = np.arange(0,len(score_mean))\n",
    "        #print(f'idx: {idx}')\n",
    "        \n",
    "#        score_std = np.std([z_scores_range,z_scores_intensity,z_scores_image], axis=0)\n",
    "#        score_std = score_std[conf_mask]\n",
    "        #print(f'score_std: {score_std}')\n",
    "#        min_index_std, min_z_score_std = min(enumerate(score_std), key=operator.itemgetter(1))\n",
    "#        max_index = idx_map[min_index_std]\n",
    "        #print(f'max_index: {max_index}')\n",
    "        \n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "     #   max_scores = np.add(np.add(z_scores_range, z_scores_intensity), z_scores_image)\n",
    "      #  max_index, max_z_score = max(enumerate(max_scores), key=operator.itemgetter(1))\n",
    "        \n",
    "        #print(f'max range: {max_z_score_range}, max intensity: {max_z_score_intensity}')\n",
    "        #max_index = max_index_range if max_z_score_range > max_z_score_intensity else max_index_intensity        \n",
    "        #max_score = max_z_score_range if max_z_score_range > max_z_score_intensity else max_z_score_intensity\n",
    "        #max_index = max_index_range\n",
    "        #max_score = max_z_score_range\n",
    "        \n",
    "        #max_index = max_index if max_score > max_z_score_image else max_index_image\n",
    "        \n",
    "        \n",
    "        #max_index = max_index_range if max_z_score_range > max_z_score_image else max_index_image\n",
    "        \n",
    "        max_index = max_index_fused\n",
    "        matching_index = nn_indices[max_index]        \n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "            #print('successful')\n",
    "        #else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_score}.')            \n",
    "            #matching_index = nn_indices[true_match_idx]\n",
    "            #dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_recall = (loc_count*1.0) / n_matches    \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/WindowedVoting', loc_precision, n_nearest_neighbors)\n",
    "    #print(f'Duration: {dur_neighbor_processing_s/n_test_set}s')    \n",
    "    #print(f'Duration S^2 Transform: {dur_s2_s/n_test_set}s')\n",
    "    #print(f'Duration Spectrum: {dur_spectrum_s/n_test_set}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9404761904761905\n",
    "0.9176470588235294\n",
    "0.8863636363636364\n",
    "0.8777777777777778\n",
    "0.8666666666666667\n",
    "\n",
    "l_max = 15/15\n",
    "0.9523809523809523\n",
    "0.9411764705882353\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8888888888888888\n",
    "\n",
    "l_max = 10/10\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8777777777777778\n",
    "0.8461538461538461\n",
    "0.8586956521739131\n",
    "0.8602150537634409\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "0.8494623655913979\n",
    "\n",
    "l_max = 7/7\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8888888888888888\n",
    "0.8571428571428571\n",
    "0.8586956521739131\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "\n",
    "lmax = 7/5\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8777777777777778\n",
    "0.8888888888888888\n",
    "0.8571428571428571\n",
    "0.8695652173913043\n",
    "0.8709677419354839\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "0.8494623655913979\n",
    "0.8421052631578947\n",
    "0.8421052631578947\n",
    "0.8421052631578947\n",
    "0.8526315789473684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1289718\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
