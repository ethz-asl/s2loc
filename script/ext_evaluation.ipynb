{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script loads the current model and performs an evaluation of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "First, initialize the model with all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import spatial\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(1)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "net = Model().cuda()\n",
    "restore = False\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "batch_size = 12\n",
    "num_workers = 12\n",
    "descriptor_size = 128\n",
    "bandwidth = 100\n",
    "net_input_size = 2*bandwidth\n",
    "n_features = 2\n",
    "cache = 50\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "writer = SummaryWriter()\n",
    "model_save = '../models/12000_b14_big.pkl'\n",
    "net.load_state_dict(torch.load(model_save))\n",
    "#summary(net, input_size=[(2, 200, 200), (2, 200, 200), (2, 200, 200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors from:\t /media/scratch/berlukas/spherical/test/anchor/\n",
      "Loading positives from:\t /media/scratch/berlukas/spherical/test/positive/\n",
      "Loading negatives from:\t /media/scratch/berlukas/spherical/test/negative/\n",
      "Done loading dataset.\n",
      "\tAnchors total: \t\t201\n",
      "\tPositives total: \t201\n",
      "\tNegatives total: \t201\n"
     ]
    }
   ],
   "source": [
    "n_data = 2000\n",
    "ds = DataSource('/media/scratch/berlukas/spherical/test', n_data, 5)\n",
    "ds.load(n_data)\n",
    "n_data = len(ds.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating anchor spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbf99eb9286491e9dbfff66a45982cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=201.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating positive spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ad4dc306e7477f944c5df828a21fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=201.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating negative spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f53abd2f814c42b77a013a472c6685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=201.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated features\n",
      "Total size:  201\n"
     ]
    }
   ],
   "source": [
    "test_set = TrainingSet(ds, restore, bandwidth)\n",
    "print(\"Total size: \", len(test_set))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, shuffle=False, num_workers=1, pin_memory=True, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    return acc\n",
    "\n",
    "net.eval()\n",
    "n_iter = 0\n",
    "anchor_embeddings = np.empty(1)\n",
    "positive_embeddings = np.empty(1)\n",
    "with torch.no_grad():\n",
    "    test_accs = AverageMeter()\n",
    "    test_pos_dist = AverageMeter()\n",
    "    test_neg_dist = AverageMeter()\n",
    "\n",
    "    for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data3.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        writer.add_scalar('Ext_Test/Loss', loss, n_iter)\n",
    "\n",
    "        acc = accuracy(dist_to_pos, dist_to_neg)\n",
    "        test_accs.update(acc, data1.size(0))\n",
    "        test_pos_dist.update(dist_to_pos.cpu().data.numpy().sum())\n",
    "        test_neg_dist.update(dist_to_neg.cpu().data.numpy().sum())\n",
    "\n",
    "        writer.add_scalar('Ext_Test/Accuracy', test_accs.avg, n_iter)\n",
    "        writer.add_scalar('Ext_Test/Distance/Positive', test_pos_dist.avg, n_iter)\n",
    "        writer.add_scalar('Ext_Test/Distance/Negative', test_neg_dist.avg, n_iter)\n",
    "\n",
    "        anchor_embeddings = np.append(anchor_embeddings, embedded_a.cpu().data.numpy().reshape([1,-1]))\n",
    "        positive_embeddings = np.append(positive_embeddings, embedded_p.cpu().data.numpy().reshape([1,-1]))\n",
    "        n_iter = n_iter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac3c1739aac4e55878e04a1c1bf42c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.43781094527363185 for 1 neighbors\n",
      "recall 0.6417910447761194 for 2 neighbors\n",
      "recall 0.7611940298507462 for 3 neighbors\n",
      "recall 0.8059701492537313 for 4 neighbors\n",
      "recall 0.835820895522388 for 5 neighbors\n",
      "recall 0.8606965174129353 for 6 neighbors\n",
      "recall 0.8706467661691543 for 7 neighbors\n",
      "recall 0.8756218905472637 for 8 neighbors\n",
      "recall 0.8855721393034826 for 9 neighbors\n",
      "recall 0.9203980099502488 for 10 neighbors\n",
      "recall 0.9353233830845771 for 11 neighbors\n",
      "recall 0.9502487562189055 for 12 neighbors\n",
      "recall 0.9601990049751243 for 13 neighbors\n",
      "recall 0.9651741293532339 for 14 neighbors\n",
      "recall 0.9751243781094527 for 15 neighbors\n",
      "recall 0.9751243781094527 for 16 neighbors\n",
      "recall 0.9800995024875622 for 17 neighbors\n",
      "recall 0.9800995024875622 for 18 neighbors\n",
      "recall 0.9800995024875622 for 19 neighbors\n",
      "recall 0.9850746268656716 for 20 neighbors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desc_anchors = anchor_embeddings[1:].reshape([n_data, descriptor_size])\n",
    "desc_positives = positive_embeddings[1:].reshape([n_data, descriptor_size])\n",
    "\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 0.05\n",
    "max_anchor_dist = 1\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):\n",
    "    pos_count = 0\n",
    "    anchor_count = 0\n",
    "    idx_count = 0\n",
    "    for idx in range(n_data):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_data):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_positives[idx,:])\n",
    "            if (dist <= max_pos_dist):\n",
    "                pos_count = pos_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_data):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_anchors[idx,:])\n",
    "            if (dist <= max_anchor_dist):\n",
    "                anchor_count = anchor_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i == idx):\n",
    "                idx_count = idx_count + 1;\n",
    "                break\n",
    "    pos_precision = (pos_count*1.0) / n_data\n",
    "    anchor_precision = (anchor_count*1.0) / n_data\n",
    "    idx_precision = (idx_count*1.0) / n_data\n",
    "    \n",
    "    print(f'recall {idx_precision} for {n_nearest_neighbors} neighbors')\n",
    "    writer.add_scalar('Ext_Test/Precision/Positive_Distance', pos_precision, n_nearest_neighbors)\n",
    "    writer.add_scalar('Ext_Test/Precision/Anchor_Distance', anchor_precision, n_nearest_neighbors)\n",
    "    writer.add_scalar('Ext_Test/Precision/Index_Count', idx_precision, n_nearest_neighbors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
