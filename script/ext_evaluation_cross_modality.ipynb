{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script loads the current model and performs an evaluation of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "First, initialize the model with all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "from mission_indices import MissionIndices\n",
    "from database_parser import DatabaseParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import pyshtools\n",
    "from pyshtools import spectralanalysis\n",
    "from pyshtools import shio\n",
    "from pyshtools import expand\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as st\n",
    "from scipy import spatial\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "n_features = 3\n",
    "bandwidth = 100\n",
    "net = Model(n_features, bandwidth).cuda()\n",
    "restore = False\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "batch_size = 12\n",
    "num_workers = 12\n",
    "descriptor_size = 256\n",
    "net_input_size = 2*bandwidth\n",
    "cache = 50\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "writer = SummaryWriter()\n",
    "stored_model = './net_params_arche_high_res_big.pkl'\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "#summary(net, input_size=[(2, 200, 200), (2, 200, 200), (2, 200, 200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(0,10,2)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading missions db from /media/scratch/berlukas/spherical/koze_high_res/missions.csv\n",
      "Read 12378 entries.\n",
      "Loading anchors from:\t/media/scratch/berlukas/spherical/koze_high_res//training_anchor_pointclouds/ and /media/scratch/berlukas/spherical/koze_high_res//training_anchor_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd93d7ee84fe4478b4803d7e088929e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623a9f4dbe9e4b1094d8113e9595910c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading positives from:\t/media/scratch/berlukas/spherical/koze_high_res//training_positive_pointclouds/ and /media/scratch/berlukas/spherical/koze_high_res//training_positive_sph_images/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16abe4fff01d46ca9727605ed1e8da82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e603123064f948782d2c16b9cad21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done loading dataset.\n",
      "\tAnchor point clouds total: \t46\n",
      "\tAnchor images total: \t\t46\n",
      "\tAnchor poses total: \t\t46\n",
      "\tPositive point clouds total: \t46\n",
      "\tPositive images total: \t\t46\n",
      "\tPositive poses total: \t\t46\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/media/scratch/berlukas/spherical/koze_high_res/\"\n",
    "#dataset_path = \"/home/berlukas/data/arche_low_res2/\"\n",
    "db_parser = DatabaseParser(dataset_path)\n",
    "\n",
    "n_test_data = 2500\n",
    "n_test_cache = n_test_data\n",
    "\n",
    "idx = np.arange(0,n_test_data, 10)\n",
    "\n",
    "ds_test = DataSource(dataset_path, n_test_cache, -1, False)\n",
    "ds_test.load(n_test_data, idx, filter_clusters=True)\n",
    "n_test_data = len(ds_test.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating features from 0 to 46\n",
      "Generating anchor spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f4a82c316a4b6d8f56c1fbefd6845b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing time in total 10.295600652694702 for 46 anchors.\n",
      "Generating positive spheres\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1112ed23b0a44cc49a6594ded551205e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=46.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing time in total 24.101486444473267 for 92 positives.\n",
      "Generated all pcl features\n",
      "Processing time in total 24.101486444473267 for 92 items.\n",
      "Processing time avg is 0.26197\n"
     ]
    }
   ],
   "source": [
    "test_set = TrainingSet(restore, bandwidth)\n",
    "test_set.generateAll(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size:  46\n"
     ]
    }
   ],
   "source": [
    "# hack for removing the images\n",
    "#test_set.anchor_features = test_set.anchor_features[:,0:2,:,:]\n",
    "#test_set.positive_features = test_set.positive_features[:,0:2,:,:]\n",
    "#test_set.negative_features = test_set.negative_features[:,0:2,:,:]\n",
    "\n",
    "\n",
    "n_test_set = len(test_set)\n",
    "print(\"Total size: \", n_test_set)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=10, shuffle=False, num_workers=1, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the descriptors for the positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "n_iter = 0\n",
    "\n",
    "anchor_embeddings = np.empty(1)\n",
    "positive_embeddings = np.empty(1)\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data1, data2) in enumerate(test_loader):\n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data2.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        \n",
    "        anchor_embeddings = np.append(anchor_embeddings, embedded_a.cpu().data.numpy().reshape([1,-1]))\n",
    "        positive_embeddings = np.append(positive_embeddings, embedded_p.cpu().data.numpy().reshape([1,-1]))\n",
    "        n_iter = n_iter + 1\n",
    "        \n",
    "desc_anchors = anchor_embeddings[1:].reshape([n_test_set, descriptor_size])        \n",
    "desc_positives = positive_embeddings[1:].reshape([n_test_set, descriptor_size])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New testing pipeline (location based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 46 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763412b77dd04b5dbe509e1e67b8fe89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall 0.34782608695652173 for 1 neighbors\n",
      "recall 0.34782608695652173 for 2 neighbors\n",
      "recall 0.3695652173913043 for 3 neighbors\n",
      "recall 0.3695652173913043 for 4 neighbors\n",
      "recall 0.3695652173913043 for 5 neighbors\n",
      "recall 0.3695652173913043 for 6 neighbors\n",
      "recall 0.3695652173913043 for 7 neighbors\n",
      "recall 0.3695652173913043 for 8 neighbors\n",
      "recall 0.3695652173913043 for 9 neighbors\n",
      "recall 0.3695652173913043 for 10 neighbors\n",
      "recall 0.3695652173913043 for 11 neighbors\n",
      "recall 0.391304347826087 for 12 neighbors\n",
      "recall 0.6956521739130435 for 13 neighbors\n",
      "recall 0.717391304347826 for 14 neighbors\n",
      "recall 0.9782608695652174 for 15 neighbors\n",
      "recall 0.9782608695652174 for 16 neighbors\n",
      "recall 1.0 for 17 neighbors\n",
      "recall 1.0 for 18 neighbors\n",
      "recall 1.0 for 19 neighbors\n",
      "recall 1.0 for 20 neighbors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_anchors)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_anchors)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "positive_poses = ds_test.positive_poses\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):    \n",
    "    loc_count = 0\n",
    "    match_count = 0\n",
    "    for idx in range(n_test_set):\n",
    "        cur_positive_pos = positive_poses[idx,5:8]\n",
    "        diff = np.subtract(anchor_poses[:,5:8], cur_positive_pos)\n",
    "        distances = np.linalg.norm(diff, axis=1)\n",
    "        if (np.count_nonzero(distances <= max_pos_dist) <= 2):            \n",
    "            continue\n",
    "        match_count = match_count + 1\n",
    "            \n",
    "        \n",
    "        nn_dists, nn_indices = tree.query(desc_positives[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(anchor_poses[nn_i,5:8], cur_positive_pos)\n",
    "            if (dist <= max_pos_dist):\n",
    "                loc_count = loc_count + 1;\n",
    "                break\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / match_count    \n",
    "    print(f'recall {loc_precision} for {n_nearest_neighbors} neighbors')\n",
    "    #print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Location', loc_precision, n_nearest_neighbors)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 58 descriptors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491df163c3e43e1ba332abb9e6d0f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3793103448275862\n",
      "0.39655172413793105\n",
      "0.6379310344827587\n",
      "0.6551724137931034\n",
      "0.6551724137931034\n",
      "0.6551724137931034\n",
      "0.6551724137931034\n",
      "0.6724137931034483\n",
      "0.6896551724137931\n",
      "0.6896551724137931\n",
      "0.6896551724137931\n",
      "0.6896551724137931\n",
      "0.6896551724137931\n",
      "0.8620689655172413\n",
      "0.9310344827586207\n",
      "0.9310344827586207\n",
      "0.9655172413793104\n",
      "0.9655172413793104\n",
      "0.9655172413793104\n",
      "0.9655172413793104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "max_anchor_dist = 1\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "positive_poses = ds_test.positive_poses\n",
    "assert len(anchor_poses) == len(positive_poses)\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):    \n",
    "    loc_count = 0\n",
    "    for idx in range(n_test_set):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                loc_count = loc_count + 1;\n",
    "                break\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_test_set    \n",
    "    #print(f'recall {loc_precision} for {n_nearest_neighbors} neighbors')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Location', loc_precision, n_nearest_neighbors)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test pipeline for a map size of 46 descriptors.\n",
      "Duration for building the kd-tree 0.002602815628051758s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f23d8b23864d2781ed7e854802af87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall loc: 0.34782608695652173 for 10 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.13043478260869565, hard: 0.17391304347826086, no 0.34782608695652173\n",
      "[FUSED] Recall loc: 0.34782608695652173, almost: 0.10869565217391304, hard: 0.17391304347826086, no 0.3695652173913043\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.34782608695652173 for 11 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.13043478260869565, hard: 0.17391304347826086, no 0.34782608695652173\n",
      "[FUSED] Recall loc: 0.34782608695652173, almost: 0.10869565217391304, hard: 0.17391304347826086, no 0.3695652173913043\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.3695652173913043 for 12 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.13043478260869565, hard: 0.15217391304347827, no 0.34782608695652173\n",
      "[FUSED] Recall loc: 0.34782608695652173, almost: 0.10869565217391304, hard: 0.17391304347826086, no 0.3695652173913043\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.6521739130434783 for 13 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.043478260869565216, hard: 0.15217391304347827, no 0.15217391304347827\n",
      "[FUSED] Recall loc: 0.6956521739130435, almost: 0.08695652173913043, hard: 0.15217391304347827, no 0.06521739130434782\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.6521739130434783 for 14 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.043478260869565216, hard: 0.17391304347826086, no 0.13043478260869565\n",
      "[FUSED] Recall loc: 0.717391304347826, almost: 0.06521739130434782, hard: 0.17391304347826086, no 0.043478260869565216\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.717391304347826 for 15 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.06521739130434782, hard: 0.10869565217391304, no 0.10869565217391304\n",
      "[FUSED] Recall loc: 0.8695652173913043, almost: 0.06521739130434782, hard: 0.06521739130434782, no 0.0\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.7391304347826086 for 16 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.06521739130434782, hard: 0.08695652173913043, no 0.10869565217391304\n",
      "[FUSED] Recall loc: 0.782608695652174, almost: 0.06521739130434782, hard: 0.08695652173913043, no 0.06521739130434782\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.7391304347826086 for 17 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.06521739130434782, hard: 0.08695652173913043, no 0.10869565217391304\n",
      "[FUSED] Recall loc: 0.7608695652173914, almost: 0.08695652173913043, hard: 0.043478260869565216, no 0.10869565217391304\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.6739130434782609 for 18 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.043478260869565216, hard: 0.10869565217391304, no 0.17391304347826086\n",
      "[FUSED] Recall loc: 0.6304347826086957, almost: 0.08695652173913043, hard: 0.10869565217391304, no 0.17391304347826086\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.6739130434782609 for 19 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.06521739130434782, hard: 0.13043478260869565, no 0.13043478260869565\n",
      "[FUSED] Recall loc: 0.6304347826086957, almost: 0.08695652173913043, hard: 0.15217391304347827, no 0.13043478260869565\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "Recall loc: 0.6739130434782609 for 20 neighbors with 46/46 correct matches.\n",
      "Remaining recall: almost: 0.06521739130434782, hard: 0.13043478260869565, no 0.13043478260869565\n",
      "[FUSED] Recall loc: 0.6521739130434783, almost: 0.08695652173913043, hard: 0.15217391304347827, no 0.10869565217391304\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_anchors)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "n_bands = 15\n",
    "tapers, eigenvalues, taper_order = spectralanalysis.SHReturnTapers(2.01, 15)\n",
    "for n_nearest_neighbors in tqdm(range(10,21)):        \n",
    "    #n_nearest_neighbors = 16\n",
    "    n_matches = 0    \n",
    "    loc_count = 0\n",
    "    almost_loc_count = 0\n",
    "    hard_loc_count = 0\n",
    "    no_loc_count = 0\n",
    "    \n",
    "    fused_loc_count = 0\n",
    "    fused_almost_loc_count = 0\n",
    "    fused_hard_loc_count = 0\n",
    "    fused_no_loc_count = 0\n",
    "    \n",
    "    final_count = 0\n",
    "    dur_neighbor_processing_s = 0\n",
    "    dur_s2_s = 0\n",
    "    dur_spectrum_s = 0\n",
    "    for idx in range(0, n_test_set):        \n",
    "    #for idx in range(0, 200):        \n",
    "        start = time.time()\n",
    "        nn_dists, nn_indices = tree.query(desc_positives[idx,:], p = p_norm, k = n_nearest_neighbors)                \n",
    "        end = time.time()\n",
    "        dur_neighbor_processing_s = dur_neighbor_processing_s + (end - start)\n",
    "\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        z_scores_fused = [0] * n_nearest_neighbors\n",
    "        z_scores_range = [0] * n_nearest_neighbors        \n",
    "        z_scores_intensity = [0] * n_nearest_neighbors\n",
    "        z_scores_image = [0] * n_nearest_neighbors                     \n",
    "        n_true_matches = 0   \n",
    "        contains_match = False\n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_test_set):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_test_set}')\n",
    "                break;\n",
    "\n",
    "            dist = spatial.distance.euclidean(anchor_poses[nn_i,5:8], positive_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True   \n",
    "                n_true_matches = n_true_matches + 1                \n",
    "\n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "            a_intensity = anchor_features[idx][1,:,:]\n",
    "            p_intensity = positive_features[nn_i][1,:,:]\n",
    "            a_img = anchor_features[idx][2,:,:]\n",
    "            p_img = positive_features[nn_i][2,:,:]\n",
    "\n",
    "            start_s2 = time.time()\n",
    "            a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "\n",
    "            a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "            p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "\n",
    "            a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "            p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "            end_s2 = time.time()\n",
    "            dur_s2_s = dur_s2_s + (end_s2 - start_s2)\n",
    "\n",
    "\n",
    "            start_spectrum = time.time()\n",
    "            saa_range = spectralanalysis.spectrum(a_range_coeffs)            \n",
    "            saa_intensity = spectralanalysis.spectrum(a_intensity_coeffs)    \n",
    "            saa_img = spectralanalysis.spectrum(a_img_coeffs)    \n",
    "            saa = np.empty([n_features, saa_range.shape[0]])\n",
    "            saa[0,:] = saa_range\n",
    "            saa[1,:] = saa_intensity\n",
    "            saa[2,:] = saa_img\n",
    "            #saa = np.mean(saa, axis=0)\n",
    "            saa = np.amax(saa, axis=0)\n",
    "\n",
    "            spp_range = spectralanalysis.spectrum(p_range_coeffs)            \n",
    "            spp_intensity = spectralanalysis.spectrum(p_intensity_coeffs)    \n",
    "            spp_img = spectralanalysis.spectrum(p_img_coeffs)    \n",
    "            spp = np.empty([n_features, spp_range.shape[0]])\n",
    "            spp[0,:] = spp_range\n",
    "            spp[1,:] = spp_intensity\n",
    "            spp[2,:] = spp_img\n",
    "            #spp = np.mean(spp, axis=0)\n",
    "            spp = np.amax(spp, axis=0)\n",
    "\n",
    "            sap_range = spectralanalysis.cross_spectrum(a_range_coeffs, p_range_coeffs)            \n",
    "            sap_intensity = spectralanalysis.cross_spectrum(a_intensity_coeffs, p_intensity_coeffs)    \n",
    "            sap_img = spectralanalysis.cross_spectrum(a_img_coeffs, p_img_coeffs)    \n",
    "            sap = np.empty([n_features, sap_range.shape[0]])\n",
    "            sap[0,:] = sap_range\n",
    "            sap[1,:] = sap_intensity\n",
    "            sap[2,:] = sap_img\n",
    "            #sap = np.mean(sap, axis=0)\n",
    "            sap = np.amax(sap, axis=0)\n",
    "\n",
    "            #saa = spectralanalysis.spectrum(a_coeffs)\n",
    "            #spp = spectralanalysis.spectrum(p_coeffs)\n",
    "            #sap = spectralanalysis.cross_spectrum(a_coeffs, p_coeffs)\n",
    "\n",
    "            #admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                                    \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap, saa, spp, tapers)\n",
    "            end_spectrum = time.time()\n",
    "            dur_spectrum_s = dur_spectrum_s + (end_spectrum - start_spectrum)            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_fused[i] = z_scores_fused[i] + score  \n",
    "                        \n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_range, saa_range, spp_range, tapers)                        \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_range[i] = z_scores_range[i] + score\n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_intensity, saa_intensity, spp_intensity, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_intensity[i] = z_scores_intensity[i] + score                           \n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_image[i] = z_scores_image[i] + score       \n",
    "            \n",
    "\n",
    "        #print(z_scores_range)\n",
    "        #print(z_scores_intensity)\n",
    "        #print(f'z_score > 2 = {np.sum(np.array(z_scores_range) > 3.8)} range, {np.sum(np.array(z_scores_intensity) > 20)} intensity')\n",
    "        #print(f'true matches: {n_true_matches}')\n",
    "\n",
    "        # normalize values\n",
    "        z_scores_fused = np.array(z_scores_fused) / (n_bands)\n",
    "        z_scores_range = np.array(z_scores_range) / (n_bands)\n",
    "        z_scores_intensity = np.array(z_scores_intensity) / (n_bands)\n",
    "        z_scores_image = np.array(z_scores_image) / (n_bands)\n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "        max_index_fused, max_z_score_fused = max(enumerate(z_scores_fused), key=operator.itemgetter(1))\n",
    "        max_index_range, max_z_score_range = max(enumerate(z_scores_range), key=operator.itemgetter(1))\n",
    "        max_index_intensity, max_z_score_intensity = max(enumerate(z_scores_intensity), key=operator.itemgetter(1))        \n",
    "        max_index_image, max_z_score_image = max(enumerate(z_scores_image), key=operator.itemgetter(1))\n",
    "        \n",
    "        \n",
    "        max_index = max_index_range if max_z_score_range > max_z_score_intensity else max_index_intensity        \n",
    "        max_score = max_z_score_range if max_z_score_range > max_z_score_intensity else max_z_score_intensity\n",
    "\n",
    "        max_index = max_index if max_score > max_z_score_image else max_index_image\n",
    "\n",
    "        matching_index = nn_indices[max_index]\n",
    "        dist = spatial.distance.euclidean(anchor_poses[matching_index,5:8], positive_poses[idx,5:8])\n",
    "        if (dist <= 5):\n",
    "            loc_count = loc_count + 1;            \n",
    "        elif (dist <= 8):\n",
    "            almost_loc_count = almost_loc_count + 1\n",
    "        elif (dist <= 11):\n",
    "            hard_loc_count = hard_loc_count + 1\n",
    "        else:\n",
    "            no_loc_count = no_loc_count + 1\n",
    "            \n",
    "        matching_index = nn_indices[max_index_fused]\n",
    "        dist = spatial.distance.euclidean(anchor_poses[matching_index,5:8], positive_poses[idx,5:8])\n",
    "        if (dist <= 5):\n",
    "            fused_loc_count = fused_loc_count + 1;            \n",
    "        elif (dist <= 8):\n",
    "            fused_almost_loc_count = fused_almost_loc_count + 1\n",
    "        elif (dist <= 11):\n",
    "            fused_hard_loc_count = fused_hard_loc_count + 1\n",
    "        else:\n",
    "            fused_no_loc_count = fused_no_loc_count + 1\n",
    "            \n",
    "        \n",
    "    \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    almost_loc_precision = (almost_loc_count*1.0) / n_matches    \n",
    "    hard_loc_precision = (hard_loc_count*1.0) / n_matches    \n",
    "    no_loc_precision = (no_loc_count*1.0) / n_matches    \n",
    "    \n",
    "    fused_loc_precision = (fused_loc_count*1.0) / n_matches    \n",
    "    fused_almost_loc_precision = (fused_almost_loc_count*1.0) / n_matches    \n",
    "    fused_hard_loc_precision = (fused_hard_loc_count*1.0) / n_matches    \n",
    "    fused_no_loc_precision = (fused_no_loc_count*1.0) / n_matches    \n",
    "    \n",
    "    print(f'Recall loc: {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_test_set} correct matches.')\n",
    "    print(f'Remaining recall: almost: {almost_loc_precision}, hard: {hard_loc_precision}, no {no_loc_precision}')\n",
    "    print(f'[FUSED] Recall loc: {fused_loc_precision}, almost: {fused_almost_loc_precision}, hard: {fused_hard_loc_precision}, no {fused_no_loc_precision}')    \n",
    "    print('-----------------------------------------------------------------------------------------------------------------')\n",
    "    #print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/WindowedVoting', loc_precision, n_nearest_neighbors)\n",
    "    #print(f'Duration: {dur_neighbor_processing_s/n_test_set}s')    \n",
    "    #print(f'Duration S^2 Transform: {dur_s2_s/n_test_set}s')\n",
    "    #print(f'Duration Spectrum: {dur_spectrum_s/n_test_set}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "tapers, eigenvalues, taper_order = spectralanalysis.SHReturnTapers(2.01, 1)\n",
    "#for n_nearest_neighbors in tqdm(range(19,20)):        \n",
    "n_nearest_neighbors = 16\n",
    "n_matches = 0    \n",
    "loc_count = 0    \n",
    "final_count = 0\n",
    "dur_neighbor_processing_s = 0\n",
    "dur_s2_s = 0\n",
    "dur_spectrum_s = 0\n",
    "for idx in range(0, n_test_set):        \n",
    "#for idx in range(0, 200):        \n",
    "    start = time.time()\n",
    "    nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)                \n",
    "    end = time.time()\n",
    "    dur_neighbor_processing_s = dur_neighbor_processing_s + (end - start)\n",
    "\n",
    "    nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "    z_scores_range = [0] * n_nearest_neighbors\n",
    "    z_scores_intensity = [0] * n_nearest_neighbors\n",
    "    z_scores_image = [0] * n_nearest_neighbors                     \n",
    "    n_true_matches = 0   \n",
    "    contains_match = False\n",
    "    for i in range(0, n_nearest_neighbors):\n",
    "        nn_i = nn_indices[i]            \n",
    "        if (nn_i >= n_test_set):\n",
    "            print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "            break;\n",
    "\n",
    "        dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            contains_match = True   \n",
    "            n_true_matches = n_true_matches + 1                \n",
    "\n",
    "        a_range = anchor_features[idx][0,:,:]\n",
    "        p_range = positive_features[nn_i][0,:,:]\n",
    "        a_intensity = anchor_features[idx][1,:,:]\n",
    "        p_intensity = positive_features[nn_i][1,:,:]\n",
    "        a_img = anchor_features[idx][2,:,:]\n",
    "        p_img = positive_features[nn_i][2,:,:]\n",
    "\n",
    "        start_s2 = time.time()\n",
    "        a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "        p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "\n",
    "        a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "        p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "\n",
    "        a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "        p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "        end_s2 = time.time()\n",
    "        dur_s2_s = dur_s2_s + (end_s2 - start_s2)\n",
    "\n",
    "\n",
    "        start_spectrum = time.time()\n",
    "        saa_range = spectralanalysis.spectrum(a_range_coeffs)            \n",
    "        saa_intensity = spectralanalysis.spectrum(a_intensity_coeffs)    \n",
    "        saa_img = spectralanalysis.spectrum(a_img_coeffs)    \n",
    "        saa = np.empty([n_features, saa_range.shape[0]])\n",
    "        saa[0,:] = saa_range\n",
    "        saa[1,:] = saa_intensity\n",
    "        saa[2,:] = saa_img\n",
    "        #saa = np.mean(saa, axis=0)\n",
    "        saa = np.amax(saa, axis=0)\n",
    "\n",
    "        spp_range = spectralanalysis.spectrum(p_range_coeffs)            \n",
    "        spp_intensity = spectralanalysis.spectrum(p_intensity_coeffs)    \n",
    "        spp_img = spectralanalysis.spectrum(p_img_coeffs)    \n",
    "        spp = np.empty([n_features, spp_range.shape[0]])\n",
    "        spp[0,:] = spp_range\n",
    "        spp[1,:] = spp_intensity\n",
    "        spp[2,:] = spp_img\n",
    "        #spp = np.mean(spp, axis=0)\n",
    "        spp = np.amax(spp, axis=0)\n",
    "\n",
    "        sap_range = spectralanalysis.cross_spectrum(a_range_coeffs, p_range_coeffs)            \n",
    "        sap_intensity = spectralanalysis.cross_spectrum(a_intensity_coeffs, p_intensity_coeffs)    \n",
    "        sap_img = spectralanalysis.cross_spectrum(a_img_coeffs, p_img_coeffs)    \n",
    "        sap = np.empty([n_features, sap_range.shape[0]])\n",
    "        sap[0,:] = sap_range\n",
    "        sap[1,:] = sap_intensity\n",
    "        sap[2,:] = sap_img\n",
    "        #sap = np.mean(sap, axis=0)\n",
    "        sap = np.amax(sap, axis=0)\n",
    "\n",
    "        #saa = spectralanalysis.spectrum(a_coeffs)\n",
    "        #spp = spectralanalysis.spectrum(p_coeffs)\n",
    "        #sap = spectralanalysis.cross_spectrum(a_coeffs, p_coeffs)\n",
    "\n",
    "        #admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                                    \n",
    "        admit, corr = spectralanalysis.SHBiasAdmitCorr(sap, saa, spp, tapers)\n",
    "        end_spectrum = time.time()\n",
    "        dur_spectrum_s = dur_spectrum_s + (end_spectrum - start_spectrum)\n",
    "\n",
    "\n",
    "        for l in range(0, 10):                \n",
    "            prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "            score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "            z_scores_intensity[i] = z_scores_intensity[i] + score  \n",
    "\n",
    "        '''                  \n",
    "\n",
    "        admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_range, saa_range, spp_range, tapers)                        \n",
    "        for l in range(0, 10):                \n",
    "            prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "            score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "            z_scores_range[i] = z_scores_range[i] + score\n",
    "\n",
    "        admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_intensity, saa_intensity, spp_intensity, tapers)                            \n",
    "        for l in range(0, 10):                \n",
    "            prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "            score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "            z_scores_intensity[i] = z_scores_intensity[i] + score                           \n",
    "\n",
    "        admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                            \n",
    "        for l in range(0, 10):                \n",
    "            prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "            score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "            z_scores_image[i] = z_scores_image[i] + score                                                                   \n",
    "        '''                  \n",
    "\n",
    "    if (contains_match is not True):            \n",
    "        continue\n",
    "\n",
    "\n",
    "    #print(z_scores_range)\n",
    "    #print(z_scores_intensity)\n",
    "    #print(f'z_score > 2 = {np.sum(np.array(z_scores_range) > 3.8)} range, {np.sum(np.array(z_scores_intensity) > 20)} intensity')\n",
    "    #print(f'true matches: {n_true_matches}')\n",
    "\n",
    "    n_matches = n_matches + 1\n",
    "    max_index_range, max_z_score_range = max(enumerate(z_scores_range), key=operator.itemgetter(1))\n",
    "    max_index_intensity, max_z_score_intensity = max(enumerate(z_scores_intensity), key=operator.itemgetter(1))        \n",
    "    max_index_image, max_z_score_image = max(enumerate(z_scores_image), key=operator.itemgetter(1))\n",
    "\n",
    "    #print(f'max range: {max_z_score_range}, max intensity: {max_z_score_intensity}')\n",
    "    max_index = max_index_range if max_z_score_range > max_z_score_intensity else max_index_intensity\n",
    "    #max_index = max_index_intensity\n",
    "    max_score = max_z_score_range if max_z_score_range > max_z_score_intensity else max_z_score_intensity\n",
    "\n",
    "    max_index = max_index if max_score > max_z_score_image else max_index_image\n",
    "\n",
    "    matching_index = nn_indices[max_index]\n",
    "    dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "    if (dist <= max_pos_dist):\n",
    "        loc_count = loc_count + 1;            \n",
    "        #print('successful')\n",
    "    #else:\n",
    "        #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_score}.')            \n",
    "        #matching_index = nn_indices[true_match_idx]\n",
    "        #dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "        #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "\n",
    "loc_recall = (loc_count*1.0) / n_matches    \n",
    "loc_precision = (loc_count*1.0) / n_matches    \n",
    "#print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "print(f'{loc_precision}')\n",
    "#writer.add_scalar('Ext_Test/Precision/WindowedVoting', loc_precision, n_nearest_neighbors)\n",
    "#print(f'Duration: {dur_neighbor_processing_s/n_test_set}s')    \n",
    "print(f'Duration S^2 Transform: {dur_s2_s/n_test_set}s')\n",
    "print(f'Duration Spectrum: {dur_spectrum_s/n_test_set}s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
