{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This script loads the current model and performs an evaluation of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize\n",
    "First, initialize the model with all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from model_relu_old import ModelOld\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "from mission_indices import MissionIndices\n",
    "from database_parser import DatabaseParser\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import pyshtools\n",
    "from pyshtools import spectralanalysis\n",
    "from pyshtools import shio\n",
    "from pyshtools import expand\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import scipy.stats as st\n",
    "from scipy import spatial\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "n_features = 3\n",
    "bandwidth = 100\n",
    "from model_relu_old import ModelOld\n",
    "net = Model(n_features, bandwidth).cuda()\n",
    "#net = ModelOld(n_features, bandwidth).cuda()\n",
    "restore = False\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "batch_size = 12\n",
    "num_workers = 12\n",
    "descriptor_size = 256\n",
    "net_input_size = 2*bandwidth\n",
    "cache = 50\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "writer = SummaryWriter()\n",
    "stored_model = './net_params_arche_high_res_big.pkl'\n",
    "net.load_state_dict(torch.load(stored_model))\n",
    "summary(net, input_size=[(3, 200, 200), (3, 200, 200), (3, 200, 200)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the data source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = \"/media/scratch/berlukas/spherical/arche_high_res2/\"\n",
    "dataset_path = \"/home/berlukas/data/arche_low_res2/\"\n",
    "db_parser = DatabaseParser(dataset_path)\n",
    "\n",
    "training_missions, test_missions = MissionIndices.get_arche_low_res()\n",
    "training_indices, test_indices = db_parser.extract_training_and_test_indices(\n",
    "    training_missions, test_missions)\n",
    "print(f'Found {len(test_missions)} test indices.')\n",
    "\n",
    "n_test_data = 10500\n",
    "n_test_cache = n_test_data\n",
    "ds_test = DataSource(dataset_path, n_test_cache, -1, False)\n",
    "idx = np.array(test_indices['idx'].tolist())\n",
    "ds_test.load(n_test_data, idx, filter_clusters=True)\n",
    "n_test_data = len(ds_test.anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TrainingSet(restore, bandwidth)\n",
    "test_set.generateAll(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hack for removing the images\n",
    "#test_set.anchor_features = test_set.anchor_features[:,0:2,:,:]\n",
    "#test_set.positive_features = test_set.positive_features[:,0:2,:,:]\n",
    "#test_set.negative_features = test_set.negative_features[:,0:2,:,:]\n",
    "\n",
    "\n",
    "n_test_set = len(test_set)\n",
    "print(\"Total size: \", n_test_set)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, num_workers=1, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ## Generate the descriptors for anchor and positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    return acc\n",
    "\n",
    "net.eval()\n",
    "n_iter = 0\n",
    "anchor_embeddings = np.empty(1)\n",
    "positive_embeddings = np.empty(1)\n",
    "with torch.no_grad():\n",
    "    test_accs = AverageMeter()\n",
    "    test_pos_dist = AverageMeter()\n",
    "    test_neg_dist = AverageMeter()\n",
    "    start = time.time()\n",
    "    for batch_idx, (data1, data2) in enumerate(test_loader):\n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data2.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        #writer.add_scalar('Ext_Test/Loss', loss, n_iter)\n",
    "\n",
    "        acc = accuracy(dist_to_pos, dist_to_neg)\n",
    "        test_accs.update(acc, data1.size(0))\n",
    "        test_pos_dist.update(dist_to_pos.cpu().data.numpy().sum())\n",
    "        test_neg_dist.update(dist_to_neg.cpu().data.numpy().sum())\n",
    "\n",
    "        #writer.add_scalar('Ext_Test/Accuracy', test_accs.avg, n_iter)\n",
    "        #writer.add_scalar('Ext_Test/Distance/Positive', test_pos_dist.avg, n_iter)\n",
    "        #writer.add_scalar('Ext_Test/Distance/Negative', test_neg_dist.avg, n_iter)\n",
    "\n",
    "        anchor_embeddings = np.append(anchor_embeddings, embedded_a.cpu().data.numpy().reshape([1,-1]))\n",
    "        positive_embeddings = np.append(positive_embeddings, embedded_p.cpu().data.numpy().reshape([1,-1]))\n",
    "        n_iter = n_iter + 1\n",
    "    end = time.time()\n",
    "    print(f'Duration per batch {(end - start)/n_test_set}s')    \n",
    "desc_anchors = anchor_embeddings[1:].reshape([n_test_set, descriptor_size])\n",
    "desc_positives = positive_embeddings[1:].reshape([n_test_set, descriptor_size])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.1377246098987184/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple old testing pipeline (index based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 0.05\n",
    "max_anchor_dist = 1\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):\n",
    "    pos_count = 0\n",
    "    anchor_count = 0\n",
    "    idx_count = 0\n",
    "    for idx in range(n_test_set):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_positives[idx,:])\n",
    "            if (dist <= max_pos_dist):\n",
    "                pos_count = pos_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(desc_positives[nn_i,:], desc_anchors[idx,:])\n",
    "            if (dist <= max_anchor_dist):\n",
    "                anchor_count = anchor_count + 1;\n",
    "                break\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i == idx):\n",
    "                idx_count = idx_count + 1;\n",
    "                break\n",
    "    pos_precision = (pos_count*1.0) / n_test_set\n",
    "    anchor_precision = (anchor_count*1.0) / n_test_set\n",
    "    idx_precision = (idx_count*1.0) / n_test_set\n",
    "    \n",
    "    print(f'recall {idx_precision} for {n_nearest_neighbors} neighbors')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Positive_Distance', pos_precision, n_nearest_neighbors)\n",
    "    #writer.add_scalar('Ext_Test/Precision/Anchor_Distance', anchor_precision, n_nearest_neighbors)\n",
    "    #writer.add_scalar('Ext_Test/Precision/Index_Count', idx_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New testing pipeline (location based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "max_anchor_dist = 1\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "positive_poses = ds_test.positive_poses\n",
    "assert len(anchor_poses) == len(positive_poses)\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,21)):    \n",
    "    loc_count = 0\n",
    "    for idx in range(0, n_test_set):\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "\n",
    "        for nn_i in nn_indices:\n",
    "            if (nn_i >= n_test_set):\n",
    "                break;\n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                loc_count = loc_count + 1;\n",
    "                break\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_test_set    \n",
    "    #print(f'recall {loc_precision} for {n_nearest_neighbors} neighbors')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Location', loc_precision, n_nearest_neighbors)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "for n_nearest_neighbors in tqdm(range(1,2)):        \n",
    "    n_matches = 0\n",
    "    loc_count = 0    \n",
    "    dur_neighbor_processing_s = 0\n",
    "    for idx in range(0, n_test_set):                \n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)\n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        \n",
    "        z_scores = [0] * n_nearest_neighbors\n",
    "        contains_match = False        \n",
    "        true_match_idx = 0\n",
    "        start = time.time()\n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_test_set):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True                \n",
    "                true_match_idx = i\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "            a_intensity = anchor_features[idx][1,:,:]\n",
    "            p_intensity = positive_features[nn_i][1,:,:]\n",
    "            #a_img = anchor_features[idx][2,:,:]\n",
    "            #p_img = positive_features[nn_i][2,:,:]\n",
    "                                               \n",
    "            a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            \n",
    "            a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "            p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "            \n",
    "            #a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "            #p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "            \n",
    "            #a_fused = np.empty([3, a_range_coeffs.shape[0], a_range_coeffs.shape[1]])\n",
    "            #p_fused = np.empty([3, p_range_coeffs.shape[0], p_range_coeffs.shape[1]])\n",
    "            #print(a_range_coeffs.shape)\n",
    "            #a_fused[0,:] = a_range_coeffs\n",
    "            \n",
    "            \n",
    "            admit, error, corr = spectralanalysis.SHAdmitCorr(a_range_coeffs, p_range_coeffs)            \n",
    "            for l in range(0, 4):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores[i] = z_scores[i] + score\n",
    "                #if math.isinf(z_scores[i]):\n",
    "                    #print(f'z-score is inf: prob = {prob}, z-score {st.norm.ppf(1-(1-prob)/2)}')\n",
    "            \n",
    "        #if (contains_match is not True):\n",
    "            #print(f'Match not found for index {idx} and {n_nearest_neighbors} neighbors')\n",
    "            #continue\n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "        max_index, max_z_score = max(enumerate(z_scores), key=operator.itemgetter(1))\n",
    "        matching_index = nn_indices[max_index]\n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "        else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_z_score}.')            \n",
    "            matching_index = nn_indices[true_match_idx]\n",
    "            dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/Voting', loc_precision, n_nearest_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Voting using Global Spectral Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Running test pipeline for a map size of {len(desc_positives)} descriptors.')\n",
    "sys.setrecursionlimit(50000)\n",
    "start = time.time()\n",
    "tree = spatial.KDTree(desc_positives)\n",
    "end = time.time()\n",
    "print(f'Duration for building the kd-tree {(end - start)}s')   \n",
    "p_norm = 2\n",
    "max_pos_dist = 5.0\n",
    "\n",
    "anchor_poses = ds_test.anchor_poses\n",
    "anchor_clouds = ds_test.anchors\n",
    "anchor_features = test_set.anchor_features\n",
    "\n",
    "positive_poses = ds_test.positive_poses\n",
    "positive_clouds = ds_test.positives\n",
    "positive_features = test_set.anchor_features\n",
    "\n",
    "tapers, eigenvalues, taper_order = spectralanalysis.SHReturnTapers(10.3, 7)\n",
    "n_bands = 7\n",
    "for n_nearest_neighbors in tqdm(range(2,21)):        \n",
    "    n_matches = 0    \n",
    "    loc_count = 0    \n",
    "    final_count = 0\n",
    "    dur_neighbor_processing_s = 0\n",
    "    dur_s2_s = 0\n",
    "    dur_spectrum_s = 0\n",
    "    for idx in range(0, n_test_set):        \n",
    "    #for idx in range(0, 100):        \n",
    "        start = time.time()\n",
    "        nn_dists, nn_indices = tree.query(desc_anchors[idx,:], p = p_norm, k = n_nearest_neighbors)                \n",
    "        end = time.time()\n",
    "        dur_neighbor_processing_s = dur_neighbor_processing_s + (end - start)\n",
    "        \n",
    "        nn_indices = [nn_indices] if n_nearest_neighbors == 1 else nn_indices\n",
    "        z_scores_fused = [0] * n_nearest_neighbors\n",
    "        z_scores_range = [0] * n_nearest_neighbors\n",
    "        z_scores_intensity = [0] * n_nearest_neighbors\n",
    "        z_scores_image = [0] * n_nearest_neighbors                     \n",
    "        n_true_matches = 0   \n",
    "        contains_match = False        \n",
    "        for i in range(0, n_nearest_neighbors):\n",
    "            nn_i = nn_indices[i]            \n",
    "            if (nn_i >= n_test_set):\n",
    "                print(f'ERROR: index {nn_i} is outside of {n_data}')\n",
    "                break;\n",
    "                \n",
    "            dist = spatial.distance.euclidean(positive_poses[nn_i,5:8], anchor_poses[idx,5:8])\n",
    "            if (dist <= max_pos_dist):\n",
    "                contains_match = True   \n",
    "                n_true_matches = n_true_matches + 1       \n",
    "                #print(f'True index = {i}')\n",
    "                \n",
    "            a_range = anchor_features[idx][0,:,:]\n",
    "            p_range = positive_features[nn_i][0,:,:]\n",
    "            a_intensity = anchor_features[idx][1,:,:]\n",
    "            p_intensity = positive_features[nn_i][1,:,:]\n",
    "            a_img = anchor_features[idx][2,:,:]\n",
    "            p_img = positive_features[nn_i][2,:,:]\n",
    "                                                 \n",
    "            start_s2 = time.time()\n",
    "            a_range_coeffs = pyshtools.expand.SHExpandDH(a_range, sampling=1)\n",
    "            p_range_coeffs = pyshtools.expand.SHExpandDH(p_range, sampling=1)\n",
    "            \n",
    "            a_intensity_coeffs = pyshtools.expand.SHExpandDH(a_intensity, sampling=1)\n",
    "            p_intensity_coeffs = pyshtools.expand.SHExpandDH(p_intensity, sampling=1)\n",
    "            \n",
    "            a_img_coeffs = pyshtools.expand.SHExpandDH(a_img, sampling=1)\n",
    "            p_img_coeffs = pyshtools.expand.SHExpandDH(p_img, sampling=1)\n",
    "            end_s2 = time.time()\n",
    "            dur_s2_s = dur_s2_s + (end_s2 - start_s2)\n",
    "                        \n",
    "                \n",
    "            start_spectrum = time.time()\n",
    "            saa_range = spectralanalysis.spectrum(a_range_coeffs)            \n",
    "            saa_intensity = spectralanalysis.spectrum(a_intensity_coeffs)    \n",
    "            saa_img = spectralanalysis.spectrum(a_img_coeffs)    \n",
    "            saa = np.empty([n_features, saa_range.shape[0]])\n",
    "            saa[0,:] = saa_range\n",
    "            saa[1,:] = saa_intensity\n",
    "            saa[2,:] = saa_img\n",
    "            #saa = np.mean(saa, axis=0)\n",
    "            saa = np.amax(saa, axis=0)\n",
    "            \n",
    "            spp_range = spectralanalysis.spectrum(p_range_coeffs)            \n",
    "            spp_intensity = spectralanalysis.spectrum(p_intensity_coeffs)    \n",
    "            spp_img = spectralanalysis.spectrum(p_img_coeffs)    \n",
    "            spp = np.empty([n_features, spp_range.shape[0]])\n",
    "            spp[0,:] = spp_range\n",
    "            spp[1,:] = spp_intensity\n",
    "            spp[2,:] = spp_img\n",
    "            #spp = np.mean(spp, axis=0)\n",
    "            spp = np.amax(spp, axis=0)\n",
    "            \n",
    "            sap_range = spectralanalysis.cross_spectrum(a_range_coeffs, p_range_coeffs)            \n",
    "            sap_intensity = spectralanalysis.cross_spectrum(a_intensity_coeffs, p_intensity_coeffs)    \n",
    "            sap_img = spectralanalysis.cross_spectrum(a_img_coeffs, p_img_coeffs)    \n",
    "            sap = np.empty([n_features, sap_range.shape[0]])\n",
    "            sap[0,:] = sap_range\n",
    "            sap[1,:] = sap_intensity\n",
    "            sap[2,:] = sap_img\n",
    "            #sap = np.mean(sap, axis=0)\n",
    "            sap = np.amax(sap, axis=0)\n",
    "            \n",
    "            #saa = spectralanalysis.spectrum(a_coeffs)\n",
    "            #spp = spectralanalysis.spectrum(p_coeffs)\n",
    "            #sap = spectralanalysis.cross_spectrum(a_coeffs, p_coeffs)\n",
    "            \n",
    "            #admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                                    \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap, saa, spp, tapers)\n",
    "            end_spectrum = time.time()\n",
    "            dur_spectrum_s = dur_spectrum_s + (end_spectrum - start_spectrum)\n",
    "            \n",
    "            \n",
    "            for l in range(0, 10):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_fused[i] = z_scores_fused[i] + score  \n",
    "            \n",
    "                             \n",
    "\n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_range, saa_range, spp_range, tapers)                        \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_range[i] = z_scores_range[i] + score\n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_intensity, saa_intensity, spp_intensity, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_intensity[i] = z_scores_intensity[i] + score                           \n",
    "            \n",
    "            admit, corr = spectralanalysis.SHBiasAdmitCorr(sap_img, saa_img, spp_img, tapers)                            \n",
    "            for l in range(0, n_bands):                \n",
    "                prob = spectralanalysis.SHConfidence(l, corr[l])                \n",
    "                score = st.norm.ppf(1-(1-prob)/2) if prob < 0.99 else 4.0\n",
    "                z_scores_image[i] = z_scores_image[i] + score                                                                   \n",
    "            \n",
    "            \n",
    "        if (contains_match is not True):            \n",
    "            continue\n",
    "        \n",
    "                \n",
    "        #print(f'z_score > 2 = {np.sum(np.array(z_scores_range) > 3.8)} range, {np.sum(np.array(z_scores_intensity) > 20)} intensity')\n",
    "        #print(f'true matches: {n_true_matches}')\n",
    "                \n",
    "        \n",
    "        # normalize values\n",
    "        z_scores_fused = np.array(z_scores_fused) / (n_bands)\n",
    "        z_scores_range = np.array(z_scores_range) / (n_bands)\n",
    "        z_scores_intensity = np.array(z_scores_intensity) / (n_bands)\n",
    "        z_scores_image = np.array(z_scores_image) / (n_bands)\n",
    "        \n",
    "#        print(z_scores_range)\n",
    " #       print(z_scores_intensity)\n",
    "  #      print(z_scores_image)\n",
    "        \n",
    "        \n",
    "        max_index_fused, max_z_score_fused = max(enumerate(z_scores_fused), key=operator.itemgetter(1))\n",
    "        max_index_range, max_z_score_range = max(enumerate(z_scores_range), key=operator.itemgetter(1))\n",
    "        max_index_intensity, max_z_score_intensity = max(enumerate(z_scores_intensity), key=operator.itemgetter(1))        \n",
    "        max_index_image, max_z_score_image = max(enumerate(z_scores_image), key=operator.itemgetter(1))\n",
    "        \n",
    "        #score_mean = np.mean([z_scores_range,z_scores_intensity,z_scores_image], axis=0)\n",
    "        #print(f'score_mean: {score_mean}')\n",
    "        #conf_mask = np.logical_or(np.logical_or(z_scores_range > 3.5, z_scores_intensity > 3.5), z_scores_image > 3.5)\n",
    "        #print(f'conf_mask: {conf_mask}')\n",
    "        #if len(score_mean[conf_mask])==0:\n",
    "#            continue\n",
    "#        idx_map = np.arange(0,len(score_mean))\n",
    "        #print(f'idx: {idx}')\n",
    "        \n",
    "#        score_std = np.std([z_scores_range,z_scores_intensity,z_scores_image], axis=0)\n",
    "#        score_std = score_std[conf_mask]\n",
    "        #print(f'score_std: {score_std}')\n",
    "#        min_index_std, min_z_score_std = min(enumerate(score_std), key=operator.itemgetter(1))\n",
    "#        max_index = idx_map[min_index_std]\n",
    "        #print(f'max_index: {max_index}')\n",
    "        \n",
    "        \n",
    "        n_matches = n_matches + 1\n",
    "     #   max_scores = np.add(np.add(z_scores_range, z_scores_intensity), z_scores_image)\n",
    "      #  max_index, max_z_score = max(enumerate(max_scores), key=operator.itemgetter(1))\n",
    "        \n",
    "        #print(f'max range: {max_z_score_range}, max intensity: {max_z_score_intensity}')\n",
    "        #max_index = max_index_range if max_z_score_range > max_z_score_intensity else max_index_intensity        \n",
    "        #max_score = max_z_score_range if max_z_score_range > max_z_score_intensity else max_z_score_intensity\n",
    "        #max_index = max_index_range\n",
    "        #max_score = max_z_score_range\n",
    "        \n",
    "        #max_index = max_index if max_score > max_z_score_image else max_index_image\n",
    "        \n",
    "        \n",
    "        #max_index = max_index_range if max_z_score_range > max_z_score_image else max_index_image\n",
    "        \n",
    "        max_index = max_index_fused\n",
    "        matching_index = nn_indices[max_index]        \n",
    "        dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], anchor_poses[idx,5:8])\n",
    "        if (dist <= max_pos_dist):\n",
    "            loc_count = loc_count + 1;            \n",
    "            #print('successful')\n",
    "        #else:\n",
    "            #print(f'Place invalid: distance anchor <-> positive: {dist} with score {max_score}.')            \n",
    "            #matching_index = nn_indices[true_match_idx]\n",
    "            #dist = spatial.distance.euclidean(positive_poses[matching_index,5:8], positive_poses[true_match_idx,5:8])\n",
    "            #print(f'Distance positive <-> true_match: {dist}, true_match score: {z_scores[true_match_idx]}')\n",
    "                \n",
    "    loc_recall = (loc_count*1.0) / n_matches    \n",
    "    loc_precision = (loc_count*1.0) / n_matches    \n",
    "    #print(f'Recall {loc_precision} for {n_nearest_neighbors} neighbors with {n_matches}/{n_data} correct matches.')\n",
    "    print(f'{loc_precision}')\n",
    "    #writer.add_scalar('Ext_Test/Precision/WindowedVoting', loc_precision, n_nearest_neighbors)\n",
    "    #print(f'Duration: {dur_neighbor_processing_s/n_test_set}s')    \n",
    "    #print(f'Duration S^2 Transform: {dur_s2_s/n_test_set}s')\n",
    "    #print(f'Duration Spectrum: {dur_spectrum_s/n_test_set}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9404761904761905\n",
    "0.9176470588235294\n",
    "0.8863636363636364\n",
    "0.8777777777777778\n",
    "0.8666666666666667\n",
    "\n",
    "l_max = 15/15\n",
    "0.9523809523809523\n",
    "0.9411764705882353\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8888888888888888\n",
    "\n",
    "l_max = 10/10\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8777777777777778\n",
    "0.8461538461538461\n",
    "0.8586956521739131\n",
    "0.8602150537634409\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "0.8494623655913979\n",
    "\n",
    "l_max = 7/7\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8888888888888888\n",
    "0.8888888888888888\n",
    "0.8571428571428571\n",
    "0.8586956521739131\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "\n",
    "lmax = 7/5\n",
    "0.9642857142857143\n",
    "0.9529411764705882\n",
    "0.9090909090909091\n",
    "0.8777777777777778\n",
    "0.8888888888888888\n",
    "0.8571428571428571\n",
    "0.8695652173913043\n",
    "0.8709677419354839\n",
    "0.8602150537634409\n",
    "0.8494623655913979\n",
    "0.8494623655913979\n",
    "0.8421052631578947\n",
    "0.8421052631578947\n",
    "0.8421052631578947\n",
    "0.8526315789473684"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
    "print(pytorch_total_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
