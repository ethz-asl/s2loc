{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2Loc Training\n",
    "\n",
    "Description: We propose to lear a descriptor of point clouds for global localization. \n",
    "\n",
    "Author: Lukas Bernreiter (lukas.bernreiter@ieee.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from loss import TripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data\n",
    "\n",
    "Load the dataset, project each point cloud on a sphere and derive a function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors from:\t /mnt/data/datasets/Spherical/training-set/training_anchor/\n",
      "Loading positives from:\t /mnt/data/datasets/Spherical/training-set/training_positive/\n",
      "Loading negatives from:\t /mnt/data/datasets/Spherical/training-set/training_negative/\n",
      "Splitting up training and testing data.\n",
      "Done loading dataset.\n",
      "\tAnchors total: \t\t100\t training/test: (80/20)\n",
      "\tPositives total: \t100\t training/test: (80/20)\n",
      "\tNegatives total: \t100\t training/test: (80/20)\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource('/mnt/data/datasets/Spherical/training-set')\n",
    "ds.load(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df95ed35a954119a14cded6afe3a4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ce2de8ed8b4cc9b683b80bca97a7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_anchor = Sphere(ds.anchors_training[0])\n",
    "len(first_anchor.point_cloud)\n",
    "\n",
    "viz = Visualize()\n",
    "viz.visualizeRawPointCloud(first_anchor, True)\n",
    "viz.visualizeSphere(first_anchor, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model and the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "net = Model().cuda()\n",
    "restore = 0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.9)\n",
    "n_epochs = 10\n",
    "batch_size = 2\n",
    "num_workers = 1\n",
    "criterion = TripletLoss(margin=2)\n",
    "\n",
    "result_save = 'triplet_result.txt'\n",
    "progress_save = 'triplet_progress.txt'\n",
    "model_save = 'net_params_new_1.pkl'\n",
    "\n",
    "fp = open(result_save,'w')\n",
    "fpp = open(progress_save, 'w')\n",
    "n_parameters = sum([p.data.nelement() for p in net.parameters()])\n",
    "fp.write('Number of params: {}\\n'.format(n_parameters))\n",
    "fp.write('features: [2, 10, 16, 20, 60]\\n')\n",
    "fp.write('bandwidths: [512, 50, 25, 15, 5]\\n')\n",
    "fp.write('batch_size = 16\\n')\n",
    "fp.write('training epoch: 20\\n')\n",
    "fp.write('TripletLoss(margin=2.0\\n')\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandwith = 100\n",
    "train_set = TrainingSet(ds, bandwith)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr=5e-3):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "if restore ==0:\n",
    "    net.train()\n",
    "\n",
    "    n_iter = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch)\n",
    "        loss_ = 0.0\n",
    "        t0 = time.time()\n",
    "        for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
    "            data1, data2, data3 = data1.cuda().float(), data2.cuda().float(), data3.cuda().float()\n",
    "            \n",
    "            embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dista, distb, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_ += loss_total.item()\n",
    "            writer.add_scalar('Loss/train', loss, n_iter)\n",
    "            n_iter = n_iter + 1\n",
    "            \n",
    "            if batch_idx % 100 == 99:\n",
    "                t1 = time.time()\n",
    "                fpp.write('%.5f\\n' %(loss_ / 100))\n",
    "                print('[Epoch %d, Batch %4d] loss: %.5f time: %.5f lr: %.3e' %\n",
    "                    (epoch + 1, batch_idx + 1, loss_ / 100, (t1-t0) / 60, lr))\n",
    "                t0 = t1\n",
    "                loss_ = 0.0\n",
    "\n",
    "    print('training finished!')\n",
    "    torch.save(net.state_dict(), model_save)\n",
    "    # validating\n",
    "    net.eval()\n",
    "\n",
    "else:\n",
    "    net.load_state_dict(torch.load(model_read))\n",
    "    net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = AverageMeter()\n",
    "test_set = TrainingSet(ds, bandwith, False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "# record the error of each triplet\n",
    "list_pos = []\n",
    "list_neg = []\n",
    "\n",
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    # print(pred)\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    # print(acc)\n",
    "    return acc\n",
    "\n",
    "def record(dista, distb):\n",
    "    list_pos.append(dista.cpu().data.numpy())\n",
    "    list_neg.append(distb.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "n_iter = 0\n",
    "for batch_idx, (data1, data2, data3) in enumerate(test_loader):\n",
    "        data1, data2, data3 = data1.cuda().float(), data2.cuda().float(), data3.cuda().float()\n",
    "        # data1, data2, data3 = Variable(data1), Variable(data2), Variable(data3)\n",
    "        embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
    "        dista, distb, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        writer.add_scalar('Loss/test', loss, n_iter)\n",
    "        n_iter = n_iter + 1\n",
    "            \n",
    "        record(dista, distb)\n",
    "\n",
    "        acc = accuracy(dista, distb)\n",
    "        accs.update(acc, data1.size(0))\n",
    "\n",
    "array_pos = np.array(list_pos)\n",
    "array_pos = array_pos.reshape(-1,)\n",
    "dataframe = pd.DataFrame(array_pos)\n",
    "dataframe.to_csv(\"pos_error.csv\", index=0)\n",
    "array_neg = np.array(list_neg)\n",
    "array_neg = array_neg.reshape(-1,)\n",
    "dataframe = pd.DataFrame(array_neg)\n",
    "dataframe.to_csv(\"neg_error.csv\", index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4ceca2dd88a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation set:  Accuracy: {:.5f}%\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation set:  Accuracy: {:.5f}%\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maccs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
     ]
    }
   ],
   "source": [
    "fp.write('Validation set:  Accuracy: {:.5f}%\\n'.format(100. * accs.avg))\n",
    "print('Validation set:  Accuracy: {:.5f}%\\n'.format(100. * accs.avg))\n",
    "fp.close()\n",
    "fpp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
