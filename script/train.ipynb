{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S2Loc Training\n",
    "\n",
    "Description: We propose to lear a descriptor of point clouds for global localization. \n",
    "\n",
    "Author: Lukas Bernreiter (lukas.bernreiter@ieee.org)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_source import DataSource\n",
    "from visualize import Visualize\n",
    "from sphere import Sphere\n",
    "from model import Model\n",
    "from loss import TripletLoss, ImprovedTripletLoss\n",
    "from training_set import TrainingSet\n",
    "from average_meter import AverageMeter\n",
    "from data_splitter import DataSplitter\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.handlers.tensorboard_logger import *\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "%matplotlib inline\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data\n",
    "\n",
    "Load the dataset, project each point cloud on a sphere and derive a function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading anchors from:\t /mnt/data/datasets/Spherical/training/training_anchor/\n",
      "Loading positives from:\t /mnt/data/datasets/Spherical/training/training_positive/\n",
      "Loading negatives from:\t /mnt/data/datasets/Spherical/training/training_negative/\n",
      "Splitting up training and testing data.\n",
      "Done loading dataset.\n",
      "\tAnchors total: \t\t100\t training/test: (100/0)\n",
      "\tPositives total: \t100\t training/test: (100/0)\n",
      "\tNegatives total: \t100\t training/test: (100/0)\n"
     ]
    }
   ],
   "source": [
    "ds = DataSource('/mnt/data/datasets/Spherical/training', 1.0)\n",
    "ds.load(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df95ed35a954119a14cded6afe3a4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ce2de8ed8b4cc9b683b80bca97a7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first_anchor = Sphere(ds.anchors_training[0])\n",
    "len(first_anchor.point_cloud)\n",
    "\n",
    "viz = Visualize()\n",
    "viz.visualizeRawPointCloud(first_anchor, True)\n",
    "viz.visualizeSphere(first_anchor, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model and the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "net = Model().cuda()\n",
    "restore = 1\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=5e-3, momentum=0.8)\n",
    "n_epochs = 10\n",
    "batch_size = 2\n",
    "num_workers = 1\n",
    "criterion = ImprovedTripletLoss(margin=2, alpha=0.5, margin2=0.2)\n",
    "\n",
    "result_save = 'triplet_result.txt'\n",
    "progress_save = 'triplet_progress.txt'\n",
    "model_save = 'net_params_new_1.pkl'\n",
    "\n",
    "fp = open(result_save,'w')\n",
    "fpp = open(progress_save, 'w')\n",
    "n_parameters = sum([p.data.nelement() for p in net.parameters()])\n",
    "fp.write('Number of params: {}\\n'.format(n_parameters))\n",
    "fp.write('features: [2, 10, 16, 20, 60]\\n')\n",
    "fp.write('bandwidths: [512, 50, 25, 15, 5]\\n')\n",
    "fp.write('batch_size = 16\\n')\n",
    "fp.write('training epoch: 20\\n')\n",
    "fp.write('TripletLoss(margin=2.0\\n')\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data...\n",
      "Generated features\n",
      "total set size:  100\n",
      "[splitter] dataset size:  100\n",
      "[splitter] test_split:  80\n",
      "[splitter] train size:  80\n",
      "[splitter] val split:  72\n",
      "train size:  36\n",
      "val size:  4\n",
      "test size:  10\n"
     ]
    }
   ],
   "source": [
    "bandwith = 100\n",
    "train_set = TrainingSet(ds, bandwith)\n",
    "print(\"total set size: \", len(train_set))\n",
    "\n",
    "split = DataSplitter(train_set, shuffle=True)\n",
    "train_loader, val_loader, test_loader = split.get_split(batch_size=batch_size, num_workers=1)\n",
    "print(\"train size: \", len(train_loader)*batch_size)\n",
    "print(\"val size: \", len(val_loader)*batch_size)\n",
    "print(\"test size: \", len(test_loader)*batch_size)\n",
    "\n",
    "#train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate_exp(optimizer, epoch_num, lr=5e-3):\n",
    "    decay_rate = 0.96\n",
    "    new_lr = lr * math.pow(decay_rate, epoch_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return new_lr\n",
    "\n",
    "val_accs = AverageMeter()\n",
    "#test_set = TrainingSet(ds, bandwith, False)\n",
    "#test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "# record the error of each triplet\n",
    "list_pos = []\n",
    "list_neg = []\n",
    "loss_ = 0\n",
    "\n",
    "def accuracy(dista, distb):\n",
    "    margin = 0\n",
    "    pred = (dista - distb - margin).cpu().data\n",
    "    # print(pred)\n",
    "    acc = ((pred < 0).sum()).float()/dista.size(0)\n",
    "    # print(acc)\n",
    "    return acc\n",
    "\n",
    "def record(dista, distb):\n",
    "    list_pos.append(dista.cpu().data.numpy())\n",
    "    list_neg.append(distb.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, criterion, optimizer, writer, epoch, n_iter, loss_, t0):\n",
    "    net.train()\n",
    "    for batch_idx, (data1, data2, data3) in enumerate(train_loader):\n",
    "        data1, data2, data3 = data1.cuda().float(), data2.cuda().float(), data3.cuda().float()        \n",
    "        embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        dista, distb, loss_triplet, loss_total = criterion(embedded_a, embedded_p, embedded_n)            \n",
    "        loss_embedd = embedded_a.norm(2) + embedded_p.norm(2) + embedded_n.norm(2)\n",
    "        loss = loss_triplet + 0.001 * loss_embedd\n",
    "        #loss = loss_triplet\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ += loss_total.item()\n",
    "\n",
    "        writer.add_scalar('Train/Loss_Triplet', loss_triplet, n_iter)\n",
    "        writer.add_scalar('Train/Loss_Embedd', loss_embedd, n_iter)\n",
    "        writer.add_scalar('Train/Loss', loss, n_iter)            \n",
    "        n_iter += 1\n",
    "\n",
    "        if batch_idx % 100 == 99:\n",
    "            t1 = time.time()\n",
    "            fpp.write('%.5f\\n' %(loss_ / 100))\n",
    "            print('[Epoch %d, Batch %4d] loss: %.5f time: %.5f lr: %.3e' %\n",
    "                (epoch + 1, batch_idx + 1, loss_ / 100, (t1-t0) / 60, lr))\n",
    "            t0 = t1\n",
    "            loss_ = 0.0\n",
    "    return n_iter\n",
    "\n",
    "def validate(net, criterion, optimizer, writer, epoch, n_iter):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data1, data2, data3) in enumerate(val_loader):\n",
    "            data1, data2, data3 = data1.cuda().float(), data2.cuda().float(), data3.cuda().float()        \n",
    "            embedded_a, embedded_p, embedded_n = net(data1, data2, data3)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            dista, distb, loss_triplet, loss_total = criterion(embedded_a, embedded_p, embedded_n)            \n",
    "            loss_embedd = embedded_a.norm(2) + embedded_p.norm(2) + embedded_n.norm(2)\n",
    "            loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "            \n",
    "            acc = accuracy(dista, distb)\n",
    "            val_accs.update(acc, data1.size(0))\n",
    "            writer.add_scalar('Validation/Loss_Triplet', loss_triplet, n_iter)\n",
    "            writer.add_scalar('Validation/Loss_Embedd', loss_embedd, n_iter)\n",
    "            writer.add_scalar('Validation/Loss', loss, n_iter)\n",
    "            writer.add_scalar('Validation/Accuracy', val_accs.avg, n_iter)\n",
    "            n_iter += 1\n",
    "    return n_iter\n",
    "\n",
    "def test(net, criterion, writer):\n",
    "    n_iter = 0\n",
    "net.eval()\n",
    "test_accs = AverageMeter()\n",
    "test_pos_dist = AverageMeter()\n",
    "test_neg_dist = AverageMeter()\n",
    "for batch_idx, (data1, data2, data3) in enumerate(test_loader):    \n",
    "        embedded_a, embedded_p, embedded_n = net(data1.cuda().float(), data2.cuda().float(), data3.cuda().float())\n",
    "        dist_to_pos, dist_to_neg, loss, loss_total = criterion(embedded_a, embedded_p, embedded_n)\n",
    "        writer.add_scalar('Test/Loss', loss, n_iter)        \n",
    "        \n",
    "        acc = accuracy(dist_to_pos, dist_to_neg)\n",
    "        test_accs.update(acc, data1.size(0))\n",
    "        test_pos_dist.update(dist_to_pos.cpu().data.numpy().sum())\n",
    "        test_neg_dist.update(dist_to_neg.cpu().data.numpy().sum())\n",
    "                \n",
    "        writer.add_scalar('Test/Accuracy', test_accs.avg, n_iter)\n",
    "        writer.add_scalar('Test/Distance/Positive', test_pos_dist.avg, n_iter)\n",
    "        writer.add_scalar('Test/Distance/Negative', test_neg_dist.avg, n_iter)        \n",
    "        \n",
    "        n_iter = n_iter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if restore ==0:    \n",
    "    train_iter = 0\n",
    "    val_iter = 0\n",
    "    loss_ = 0.0\n",
    "    for epoch in range(n_epochs):\n",
    "        lr = adjust_learning_rate_exp(optimizer, epoch_num=epoch)\n",
    "        t0 = time.time()\n",
    "        \n",
    "        train_iter = train(net, criterion, optimizer, writer, epoch, train_iter, loss_, t0)\n",
    "        \n",
    "        val_iter = validate(net, criterion, optimizer, writer, epoch, val_iter)\n",
    "        \n",
    "        writer.add_scalar('Train/lr', lr, epoch)\n",
    "\n",
    "    print('training finished!')\n",
    "    torch.save(net.state_dict(), model_save)    \n",
    "else:\n",
    "    net.load_state_dict(torch.load(model_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg:  1.6161479949951172\n",
      "avg:  1.054598867893219\n",
      "avg:  0.795014480749766\n",
      "avg:  0.6495255194604397\n",
      "avg:  0.6584954112768173\n",
      "avg:  0.6284894123673439\n",
      "avg:  0.6578867925064904\n",
      "avg:  0.8956968914717436\n",
      "avg:  0.8271098683277766\n",
      "avg:  0.7953274920582771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\narray_pos = np.array(list_pos)\\narray_pos = array_pos.reshape(-1,)\\ndataframe = pd.DataFrame(array_pos)\\ndataframe.to_csv(\"pos_error.csv\", index=0)\\narray_neg = np.array(list_neg)\\narray_neg = array_neg.reshape(-1,)\\ndataframe = pd.DataFrame(array_neg)\\ndataframe.to_csv(\"neg_error.csv\", index=0)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()\n",
    "fp.close()\n",
    "fpp.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
